{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMU Validation Study Tibia Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMeasureU Blue Thunder Specs:\n",
    "- Sampling freq for lowg = 500hz\n",
    "- 5 minutes of data = 337,500 rows (1125hz) or 478,000 (1600hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom functions ---\n",
    "import functions.file_import_gui as gui\n",
    "import functions.data_prep as prep\n",
    "import functions.custom_plots as plots\n",
    "import functions.peak_detection as peaks\n",
    "import functions.stats as stats\n",
    "import functions.stride_variables as stride\n",
    "\n",
    "# For saving files\n",
    "import os\n",
    "\n",
    "# For dataframes ---\n",
    "import pandas as pd\n",
    "\n",
    "# For plotting ---\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For DFA analysis\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject to process\n",
    "sub_id = 'imu_val_002'\n",
    "time_pt = 'time2'\n",
    "run_type = 'run'\n",
    "\n",
    "# low g (500hz) ---\n",
    "# set directory\n",
    "initialdir = f\"data/five_min_runs/imu_validation_study/{sub_id}/{time_pt}/left_tibia/{run_type}\"\n",
    "# bring in csv files with data\n",
    "dfs_lt_lowg, keys_list = gui.read_csv_files_gui(initialdir)\n",
    "# set directory\n",
    "initialdir = f\"data/five_min_runs/imu_validation_study/{sub_id}/{time_pt}/right_tibia/{run_type}\"\n",
    "# bring in csv files with data\n",
    "dfs_rt_lowg, keys_list = gui.read_csv_files_gui(initialdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep ---\n",
    "\n",
    "# crop data for 5 mins (removes extra rows at the beginning so that there are exactly 5 mins of data)\n",
    "prep.crop_df_five_mins(dfs_lt_lowg, sample_freq = 500)\n",
    "prep.crop_df_five_mins(dfs_rt_lowg, sample_freq = 500)\n",
    "\n",
    "# calculate and add resultant column\n",
    "prep.add_resultant_column(dfs_lt_lowg, column_x = 'accel_x (m/s2)', column_y = 'accel_y (m/s2)', column_z = 'accel_z (m/s2)', name_of_res_column = 'res_m/s/s')\n",
    "prep.add_resultant_column(dfs_rt_lowg, column_x = 'accel_x (m/s2)', column_y = 'accel_y (m/s2)', column_z = 'accel_z (m/s2)', name_of_res_column = 'res_m/s/s')\n",
    "\n",
    "# convert accel columns to gs\n",
    "prep.accel_to_gs_columns(dfs_lt_lowg, column_x = 'accel_x (m/s2)', column_y = 'accel_y (m/s2)', column_z = 'accel_z (m/s2)', name_of_res_column = 'res_m/s/s')\n",
    "prep.accel_to_gs_columns(dfs_rt_lowg, column_x = 'accel_x (m/s2)', column_y = 'accel_y (m/s2)', column_z = 'accel_z (m/s2)', name_of_res_column = 'res_m/s/s')\n",
    "\n",
    "# shift time scale to start at 0\n",
    "prep.shift_time_s_to_zero(dfs_lt_lowg, time_col='timestamp')\n",
    "prep.shift_time_s_to_zero(dfs_rt_lowg, time_col='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tibial Acceleration of Resultant\n",
    "- AVG Peak Tibial Accleration - Finds peaks of the resultant and then calculates the average from these peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for functions below\n",
    "# The minimum time between peaks i.e. footstrikes corresponding to 0.50 secs for running and 0.70 secs for walking\n",
    "\n",
    "# running (250) / walking (350)\n",
    "if run_type == \"run\":\n",
    "    lowg_time_between_peaks = 250\n",
    "elif run_type == \"walk\":\n",
    "    lowg_time_between_peaks = 350\n",
    "else:\n",
    "    raise ValueError(\"Invalid run_type. Please set run_type to 'run' or 'walk'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1:\n",
    "\n",
    "# Find peaks with no thresholds\n",
    "# NOTE: This will be used to then determine individual thresholds for max peak height\n",
    "\n",
    "# lowg ---\n",
    "# left\n",
    "_, dfs_lt_res_peak_values_lowg_no_threshold = peaks.calc_avg_positive_peaks(\n",
    "    dfs_lt_lowg, columns=['res_g'], time_column='time_s_scaled', \n",
    "    min_peak_height=None, max_peak_height=None,\n",
    "    min_samples_between_peaks=lowg_time_between_peaks\n",
    ")\n",
    "# right\n",
    "_, dfs_rt_res_peak_values_lowg_no_threshold = peaks.calc_avg_positive_peaks(\n",
    "    dfs_rt_lowg, columns=['res_g'], time_column='time_s_scaled', \n",
    "    min_peak_height=None, max_peak_height=None,\n",
    "    min_samples_between_peaks=lowg_time_between_peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: \n",
    "\n",
    "# Determine subject's individual thresholds for max_peak_height and min_peak_height from peaks indentified w/ no threshold\n",
    "\n",
    "k = 4 #IQR \n",
    "z = 4 #SDs\n",
    "\n",
    "# lowg ---\n",
    "summary_tbl_lt_lowg = stats.create_summary_tbl(dfs_lt_res_peak_values_lowg_no_threshold, ['peak_values'], k=k, z=z)\n",
    "summary_tbl_rt_lowg = stats.create_summary_tbl(dfs_rt_res_peak_values_lowg_no_threshold, ['peak_values'], k=k, z=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3:\n",
    "\n",
    "# Use individualized max and min peak height threshold as upper limit for finding peaks\n",
    "# NOTE: This uses a different peak function that takes the summary table as inputs and steps through the indiv rows of each run/sensor\n",
    "\n",
    "# lowg ---\n",
    "# left\n",
    "lt_res_peak_accel_lowg_df, dfs_lt_res_peak_values_lowg_threshold = peaks.calc_avg_positive_peaks_from_tbl(\n",
    "    dfs_lt_lowg, ['res_g'], time_column='time_s_scaled',\n",
    "    summary_table=summary_tbl_lt_lowg, id_column=\"id\", min_peak_height_column=\"lower_bound_k\", max_peak_height_column=\"upper_bound_k\",\n",
    "    min_samples_between_peaks=lowg_time_between_peaks\n",
    "    )\n",
    "\n",
    "# right\n",
    "rt_res_peak_accel_lowg_df, dfs_rt_res_peak_values_lowg_threshold = peaks.calc_avg_positive_peaks_from_tbl(\n",
    "    dfs_rt_lowg, ['res_g'], time_column='time_s_scaled',\n",
    "    summary_table=summary_tbl_rt_lowg, id_column=\"id\", min_peak_height_column=\"lower_bound_k\", max_peak_height_column=\"upper_bound_k\",\n",
    "    min_samples_between_peaks=lowg_time_between_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually Inspect Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot keys for specific runs to visualize below ---\n",
    "\n",
    "# Grab all keys from the dictionaries\n",
    "dfs_lt_lowg_plots = {key: value for key, value in dfs_lt_lowg.items()}\n",
    "dfs_rt_lowg_plots = {key: value for key, value in dfs_rt_lowg.items()}\n",
    "\n",
    "# Create and store plots for specified columns ---\n",
    "\n",
    "x_col = 'time_s_scaled'\n",
    "y_cols = ['res_g']\n",
    "\n",
    "line_plots_lt_lowg = plots.create_line_plots(dfs_lt_lowg_plots, x_col, y_cols)\n",
    "line_plots_rt_lowg = plots.create_line_plots(dfs_rt_lowg_plots, x_col, y_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowg ---\n",
    "\n",
    "# left\n",
    "peaks_to_plot = dfs_lt_res_peak_values_lowg_threshold\n",
    "\n",
    "# Iterate over the keys in dfs_lt_lowg\n",
    "for key in dfs_lt_lowg:\n",
    "    fig = line_plots_lt_lowg.get(key)\n",
    "    if fig is not None:\n",
    "        # If 'peaks' trace already exists, remove it before adding new one\n",
    "        fig.data = [trace for trace in fig.data if trace.name != 'peaks']\n",
    "\n",
    "        # Check if key exists in peaks_to_plot\n",
    "        if key in peaks_to_plot:\n",
    "            # Get corresponding DataFrame\n",
    "            df_lowg_peaks = peaks_to_plot[key]\n",
    "            # Add points to figure\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df_lowg_peaks['time_s_scaled'], \n",
    "                y=df_lowg_peaks['peak_values'], \n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color='black',  # for example, choose a color that stands out\n",
    "                ),\n",
    "                name='peaks'  # you can name the trace to be referenced in legend\n",
    "            ))\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(f\"No plot found with key {key}\")\n",
    "\n",
    "# right\n",
    "peaks_to_plot = dfs_rt_res_peak_values_lowg_threshold\n",
    "\n",
    "# Iterate over the keys in dfs_rt_lowg\n",
    "for key in dfs_rt_lowg:\n",
    "    fig = line_plots_rt_lowg.get(key)\n",
    "    if fig is not None:\n",
    "        # If 'peaks' trace already exists, remove it before adding new one\n",
    "        fig.data = [trace for trace in fig.data if trace.name != 'peaks']\n",
    "\n",
    "        # Check if key exists in peaks_to_plot\n",
    "        if key in peaks_to_plot:\n",
    "            # Get corresponding DataFrame\n",
    "            df_lowg_peaks = peaks_to_plot[key]\n",
    "            # Add points to figure\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df_lowg_peaks['time_s_scaled'], \n",
    "                y=df_lowg_peaks['peak_values'], \n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color='black',  # for example, choose a color that stands out\n",
    "                ),\n",
    "                name='peaks'  # you can name the trace to be referenced in legend\n",
    "            ))\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(f\"No plot found with key {key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Variables to Excel Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add suffix to varible names\n",
    "\n",
    "# left\n",
    "lt_res_peak_accel_lowg_df['variable'] = lt_res_peak_accel_lowg_df['variable'] + '_lt_500hz'\n",
    "# right\n",
    "rt_res_peak_accel_lowg_df['variable'] = rt_res_peak_accel_lowg_df['variable'] + '_rt_500hz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table to export & append rows to to my processed variables table in Excel ---\n",
    "\n",
    "# left\n",
    "df_to_export = prep.export_tbl_imu_val(lt_res_peak_accel_lowg_df)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)\n",
    "# right\n",
    "df_to_export = prep.export_tbl_imu_val(rt_res_peak_accel_lowg_df)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride Time\n",
    "\n",
    "Calculate Stride Times (STs) using the time between each consecutive footstrikes of single leg  (time of step 2 - time of step 1, time of step 3 - time of step 2, etc.)\n",
    " - Mean\n",
    " - Standard Deviation (SD)\n",
    " - Coefficient of Variance (CV)\n",
    " - Strides per min (SPM)\n",
    " - Fractal Scaling Index (FSI) via Detrended Fluctuation Analysis (DFA)\n",
    "\n",
    " NOTE: SPM needs to be doubled (it is reflective of just one leg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the time column where each peak occured, calculate the difference between consecutive foot strikes\n",
    "# Creates a new column called stride_times for each run\n",
    "\n",
    "# left\n",
    "dfs_lt_stride_times_lowg = stride.calc_stride_times(dfs=dfs_lt_res_peak_values_lowg_threshold , time_column=\"time_s_scaled\")\n",
    "# right\n",
    "dfs_rt_stride_times_lowg = stride.calc_stride_times(dfs=dfs_rt_res_peak_values_lowg_threshold , time_column=\"time_s_scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table for stride times\n",
    "\n",
    "k = 3 #IQR \n",
    "z = 3 #SDs\n",
    "\n",
    "st_summary_tbl_lt_lowg = stats.create_summary_tbl(dfs_lt_stride_times_lowg, ['stride_times'], k=k, z=z)\n",
    "st_summary_tbl_rt_lowg = stats.create_summary_tbl(dfs_rt_stride_times_lowg, ['stride_times'], k=k, z=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stride time outliers based on the upper and lower threshold values created in the summary table above\n",
    "\n",
    "# left\n",
    "dfs_lt_stride_times_lowg_no_outliers, counts_dfs_lt_stride_times_lowg = stats.remove_outliers(\n",
    "    dfs_lt_stride_times_lowg, 'stride_times', \n",
    "    st_summary_tbl_lt_lowg, id_column='id', lower_threshold_column='lower_bound_k', upper_threshold_column='upper_bound_k')\n",
    "# right\n",
    "dfs_rt_stride_times_lowg_no_outliers, counts_dfs_rt_stride_times_lowg = stats.remove_outliers(\n",
    "    dfs_rt_stride_times_lowg, 'stride_times', \n",
    "    st_summary_tbl_rt_lowg, id_column='id', lower_threshold_column='lower_bound_k', upper_threshold_column='upper_bound_k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate stride time variables \n",
    "# Returns a single table with the columns key, variable, and value\n",
    "\n",
    "# left\n",
    "st_vars_lt_lowg_df = stride.calc_stride_times_vars(dfs_lt_stride_times_lowg,'stride_times', total_run_time_mins=5)\n",
    "# right\n",
    "st_vars_rt_lowg_df = stride.calc_stride_times_vars(dfs_rt_stride_times_lowg, 'stride_times', total_run_time_mins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Variables to Excel Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add suffix to varible names\n",
    "\n",
    "# left\n",
    "st_vars_lt_lowg_df['variable'] = st_vars_lt_lowg_df['variable'] + '_lt_500hz'\n",
    "# right\n",
    "st_vars_rt_lowg_df['variable'] = st_vars_rt_lowg_df['variable'] + '_rt_500hz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table to export & append rows to to my processed variables table in Excel ---\n",
    "\n",
    "# left\n",
    "df_to_export = prep.export_tbl_imu_val(st_vars_lt_lowg_df)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)\n",
    "# right\n",
    "df_to_export = prep.export_tbl_imu_val(st_vars_rt_lowg_df)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imu_training_load_study-UQqD6JkG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
