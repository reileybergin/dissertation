{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMU Tibia Left & Right Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specs:\n",
    "- IMeasureU Blue Trident Senors\n",
    "- Sampling freq for lowg = 1125hz\n",
    "- Sampling freq for highg = 1600hz\n",
    "- 5 minutes of data = 337,500 rows (1125hz) or 478,000 (1600hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom functions ---\n",
    "import functions.file_import_gui as gui\n",
    "import functions.data_prep as prep\n",
    "import functions.custom_plots as plots\n",
    "import functions.peak_detection as peaks\n",
    "import functions.stats as stats\n",
    "import functions.stride_variables as stride\n",
    "\n",
    "# For saving files\n",
    "import os\n",
    "\n",
    "# For dataframes ---\n",
    "import pandas as pd\n",
    "\n",
    "# For plotting ---\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For DFA analysis\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in IMU data files ---\n",
    "\n",
    "# Subject to process\n",
    "sub_id = 'run014'\n",
    "\n",
    "# low g ---\n",
    "# set directory\n",
    "initialdir = f\"data/five_min_runs/{sub_id}/lowg_1125hz/left_tibia\"\n",
    "# bring in csv files with data\n",
    "dfs_lt_lowg, keys_list = gui.read_csv_files_gui(initialdir)\n",
    "# set directory\n",
    "initialdir = f\"data/five_min_runs/{sub_id}/lowg_1125hz/right_tibia\"\n",
    "# bring in csv files with data\n",
    "dfs_rt_lowg, keys_list = gui.read_csv_files_gui(initialdir)\n",
    "\n",
    "# highg --\n",
    "# set directory\n",
    "initialdir = f\"data/five_min_runs/{sub_id}/highg_1600hz/left_tibia\"\n",
    "# bring in csv files with data\n",
    "dfs_lt_highg, keys_list = gui.read_csv_files_gui(initialdir)\n",
    "# set directory\n",
    "initialdir = f\"data/five_min_runs/{sub_id}/highg_1600hz/right_tibia\"\n",
    "# bring in csv files with data\n",
    "dfs_rt_highg, keys_list = gui.read_csv_files_gui(initialdir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep ---\n",
    "\n",
    "# lowg ---\n",
    "# crop data for 5 mins (removes extra rows at the beginning so that there are exactly 5 mins of data)\n",
    "prep.crop_df_five_mins(dfs_lt_lowg, sample_freq = 1125)\n",
    "prep.crop_df_five_mins(dfs_rt_lowg, sample_freq = 1125)\n",
    "# calculate and add resultant column\n",
    "prep.add_resultant_column(dfs_lt_lowg, column_x = 'ax_m/s/s', column_y = 'ay_m/s/s', column_z = 'az_m/s/s', name_of_res_column = 'res_m/s/s')\n",
    "prep.add_resultant_column(dfs_rt_lowg, column_x = 'ax_m/s/s', column_y = 'ay_m/s/s', column_z = 'az_m/s/s', name_of_res_column = 'res_m/s/s')\n",
    "# convert accel columns to gs\n",
    "prep.accel_to_gs_columns(dfs_lt_lowg)\n",
    "prep.accel_to_gs_columns(dfs_rt_lowg)\n",
    "# shift time scale to start at 0\n",
    "prep.shift_time_s_to_zero(dfs_lt_lowg)\n",
    "prep.shift_time_s_to_zero(dfs_rt_lowg)\n",
    "\n",
    "# highg ---\n",
    "# crop data for 5 mins (removes extra rows at the beginning so that there are exactly 5 mins of data)\n",
    "prep.crop_df_five_mins(dfs_lt_highg, sample_freq = 1600)\n",
    "prep.crop_df_five_mins(dfs_rt_highg, sample_freq = 1600)\n",
    "# calculate and add resultant column\n",
    "prep.add_resultant_column(dfs_lt_highg, column_x = 'ax_m/s/s', column_y = 'ay_m/s/s', column_z = 'az_m/s/s', name_of_res_column = 'res_m/s/s')\n",
    "prep.add_resultant_column(dfs_rt_highg, column_x = 'ax_m/s/s', column_y = 'ay_m/s/s', column_z = 'az_m/s/s', name_of_res_column = 'res_m/s/s')\n",
    "# convert accel columns to gs\n",
    "prep.accel_to_gs_columns(dfs_lt_highg)\n",
    "prep.accel_to_gs_columns(dfs_rt_highg)\n",
    "# shift time scale to start at 0\n",
    "prep.shift_time_s_to_zero(dfs_lt_highg)\n",
    "prep.shift_time_s_to_zero(dfs_rt_highg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create plots to save in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and store plots for specified columns ---\n",
    "\n",
    "x_col = 'time_s_scaled'\n",
    "y_cols = ['ax_g', 'ay_g', 'az_g', 'res_g']\n",
    "\n",
    "# lowg ---\n",
    "line_plots_lt_lowg = plots.create_line_plots(dfs_lt_lowg, x_col, y_cols)\n",
    "line_plots_rt_lowg = plots.create_line_plots(dfs_rt_lowg, x_col, y_cols)\n",
    "\n",
    "# highg ---\n",
    "line_plots_lt_highg = plots.create_line_plots(dfs_lt_highg, x_col, y_cols)\n",
    "line_plots_rt_highg = plots.create_line_plots(dfs_rt_highg, x_col, y_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save plots to folder ---\n",
    "\n",
    "# lowg --\n",
    "\n",
    "# left\n",
    "# set directory\n",
    "output_dir = f\"plots/{sub_id}/left_tibia/lowg_1125hz/\"\n",
    "# check if directory exists, if not, create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "# loop through all plots and save them\n",
    "for key, fig in line_plots_lt_lowg.items():\n",
    "    file_path = os.path.join(output_dir, f\"{key}.html\")\n",
    "    pio.write_html(fig, file_path)\n",
    "\n",
    "# right\n",
    "# set directory\n",
    "output_dir = f\"plots/{sub_id}/right_tibia/lowg_1125hz/\"\n",
    "# check if directory exists, if not, create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "# loop through all plots and save them\n",
    "for key, fig in line_plots_rt_lowg.items():\n",
    "    file_path = os.path.join(output_dir, f\"{key}.html\")\n",
    "    pio.write_html(fig, file_path)\n",
    "\n",
    "# highg ---\n",
    "\n",
    "# left\n",
    "# set directory\n",
    "output_dir = f\"plots/{sub_id}/left_tibia/highg_1600hz/\"\n",
    "# check if directory exists, if not, create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "# loop through all plots and save them\n",
    "for key, fig in line_plots_lt_highg.items():\n",
    "    file_path = os.path.join(output_dir, f\"{key}.html\")\n",
    "    pio.write_html(fig, file_path)\n",
    "\n",
    "# right\n",
    "# set directory\n",
    "output_dir = f\"plots/{sub_id}/right_tibia/highg_1600hz/\"\n",
    "# check if directory exists, if not, create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "# loop through all plots and save them\n",
    "for key, fig in line_plots_rt_highg.items():\n",
    "    file_path = os.path.join(output_dir, f\"{key}.html\")\n",
    "    pio.write_html(fig, file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tibial Acceleration of Resultant\n",
    "- AVG Peak Tibial Accleration - Finds peaks of the resultant and then calculates the average from these peaks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for functions below\n",
    "# The minimum time between peaks i.e. footstrikes corresponding to 0.50 secs\n",
    "lowg_time_between_peaks = 562\n",
    "highg_time_between_peaks = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1:\n",
    "\n",
    "# Find peaks with no thresholds\n",
    "# NOTE: This will be used to then determine individual thresholds for max peak height\n",
    "\n",
    "# lowg ---\n",
    "# left\n",
    "_, dfs_lt_res_peak_values_lowg_no_threshold = peaks.calc_avg_positive_peaks(\n",
    "    dfs_lt_lowg, columns=['res_g'], time_column='time_s_scaled', \n",
    "    min_peak_height=None, max_peak_height=None,\n",
    "    min_samples_between_peaks=lowg_time_between_peaks\n",
    ")\n",
    "# right\n",
    "_, dfs_rt_res_peak_values_lowg_no_threshold = peaks.calc_avg_positive_peaks(\n",
    "    dfs_rt_lowg, columns=['res_g'], time_column='time_s_scaled', \n",
    "    min_peak_height=None, max_peak_height=None,\n",
    "    min_samples_between_peaks=lowg_time_between_peaks)\n",
    "\n",
    "# highg ---\n",
    "# left\n",
    "_, dfs_lt_res_peak_values_highg_no_threshold = peaks.calc_avg_positive_peaks(\n",
    "    dfs_lt_highg, columns=['res_g'], time_column='time_s_scaled', \n",
    "    min_peak_height=None, max_peak_height=None,\n",
    "    min_samples_between_peaks=highg_time_between_peaks\n",
    ")\n",
    "# right\n",
    "_, dfs_rt_res_peak_values_highg_no_threshold = peaks.calc_avg_positive_peaks(\n",
    "    dfs_rt_highg, columns=['res_g'], time_column='time_s_scaled', \n",
    "    min_peak_height=None, max_peak_height=None,\n",
    "    min_samples_between_peaks=highg_time_between_peaks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: \n",
    "\n",
    "# Determine subject's individual thresholds for max_peak_height and min_peak_height from peaks indentified w/ no threshold\n",
    "\n",
    "k = 4 #IQR \n",
    "z = 4 #SDs\n",
    "\n",
    "# lowg ---\n",
    "summary_tbl_lt_lowg = stats.create_summary_tbl(dfs_lt_res_peak_values_lowg_no_threshold, ['peak_values'], k=k, z=z)\n",
    "summary_tbl_rt_lowg = stats.create_summary_tbl(dfs_rt_res_peak_values_lowg_no_threshold, ['peak_values'], k=k, z=z)\n",
    "\n",
    "# highg ---\n",
    "summary_tbl_lt_highg = stats.create_summary_tbl(dfs_lt_res_peak_values_highg_no_threshold, ['peak_values'], k=k, z=z)\n",
    "summary_tbl_rt_highg = stats.create_summary_tbl(dfs_rt_res_peak_values_highg_no_threshold, ['peak_values'], k=k, z=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3:\n",
    "\n",
    "# Use individualized max and min peak height threshold as upper limit for finding peaks\n",
    "# NOTE: This uses a different peak function that takes the summary table as inputs and steps through the indiv rows of each run/sensor\n",
    "\n",
    "# lowg ---\n",
    "# left\n",
    "lt_res_peak_accel_lowg_df, dfs_lt_res_peak_values_lowg_threshold = peaks.calc_avg_positive_peaks_from_tbl(\n",
    "    dfs_lt_lowg, ['res_g'], time_column='time_s_scaled',\n",
    "    summary_table=summary_tbl_lt_lowg, id_column=\"id\", min_peak_height_column=\"lower_bound_k\", max_peak_height_column=\"upper_bound_k\",\n",
    "    min_samples_between_peaks=lowg_time_between_peaks\n",
    "    )\n",
    "\n",
    "# right\n",
    "rt_res_peak_accel_lowg_df, dfs_rt_res_peak_values_lowg_threshold = peaks.calc_avg_positive_peaks_from_tbl(\n",
    "    dfs_rt_lowg, ['res_g'], time_column='time_s_scaled',\n",
    "    summary_table=summary_tbl_rt_lowg, id_column=\"id\", min_peak_height_column=\"lower_bound_k\", max_peak_height_column=\"upper_bound_k\",\n",
    "    min_samples_between_peaks=lowg_time_between_peaks\n",
    "    )\n",
    "\n",
    "# highg ---\n",
    "# left\n",
    "lt_res_peak_accel_highg_df, dfs_lt_res_peak_values_highg_threshold = peaks.calc_avg_positive_peaks_from_tbl(\n",
    "    dfs_lt_highg, ['res_g'], time_column='time_s_scaled',\n",
    "    summary_table=summary_tbl_lt_highg, id_column=\"id\", min_peak_height_column=\"lower_bound_k\", max_peak_height_column=\"upper_bound_k\",\n",
    "    min_samples_between_peaks=highg_time_between_peaks\n",
    "    )\n",
    "\n",
    "# right\n",
    "rt_res_peak_accel_highg_df, dfs_rt_res_peak_values_highg_threshold = peaks.calc_avg_positive_peaks_from_tbl(\n",
    "    dfs_rt_highg, ['res_g'], time_column='time_s_scaled', \n",
    "    summary_table=summary_tbl_rt_highg, id_column=\"id\", min_peak_height_column=\"lower_bound_k\", max_peak_height_column=\"upper_bound_k\",\n",
    "    min_samples_between_peaks=highg_time_between_peaks\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually Inspect Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot keys for specific runs to visualize below ---\n",
    "\n",
    "# lowg ---\n",
    "# left\n",
    "plot_keys_lt_lowg = [\n",
    "    f'{sub_id}_light_prs_pre_00917_lowg', \n",
    "    f'{sub_id}_heavy_prs_pre_00917_lowg', \n",
    "]\n",
    "# right\n",
    "plot_keys_rt_lowg = [\n",
    "    f'{sub_id}_light_prs_pre_00925_lowg',\n",
    "    f'{sub_id}_heavy_prs_pre_00925_lowg'\n",
    "]\n",
    "\n",
    "# highg ---\n",
    "# left\n",
    "plot_keys_lt_highg = [\n",
    "    f'{sub_id}_light_prs_pre_00917_highg', \n",
    "    f'{sub_id}_heavy_prs_pre_00917_highg', \n",
    "]\n",
    "# right\n",
    "plot_keys_rt_highg = [\n",
    "    f'{sub_id}_light_prs_pre_00925_highg',\n",
    "    f'{sub_id}_heavy_prs_pre_00925_highg'\n",
    "]\n",
    "\n",
    "# Create dictionary with dfs I want to plot\n",
    "\n",
    "# lowg ---\n",
    "dfs_lt_lowg_plots = {key: dfs_lt_lowg[key] for key in plot_keys_lt_lowg if key in dfs_lt_lowg}\n",
    "dfs_rt_lowg_plots = {key: dfs_rt_lowg[key] for key in plot_keys_rt_lowg if key in dfs_rt_lowg}\n",
    "\n",
    "# highg ---\n",
    "dfs_lt_highg_plots = {key: dfs_lt_highg[key] for key in plot_keys_lt_highg if key in dfs_lt_highg}\n",
    "dfs_rt_highg_plots = {key: dfs_rt_highg[key] for key in plot_keys_rt_highg if key in dfs_rt_highg}\n",
    "\n",
    "# Create and store plots for specified columns ---\n",
    "\n",
    "x_col = 'time_s_scaled'\n",
    "y_cols = ['res_g']\n",
    "\n",
    "# lowg ---\n",
    "line_plots_lt_lowg = plots.create_line_plots(dfs_lt_lowg_plots, x_col, y_cols)\n",
    "line_plots_rt_lowg = plots.create_line_plots(dfs_rt_lowg_plots, x_col, y_cols)\n",
    "\n",
    "# highg ---\n",
    "line_plots_lt_highg = plots.create_line_plots(dfs_lt_highg_plots, x_col, y_cols)\n",
    "line_plots_rt_highg = plots.create_line_plots(dfs_rt_highg_plots, x_col, y_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now actually plot the data ---\n",
    "\n",
    "# lowg ---\n",
    "# left\n",
    "peaks_to_plot = dfs_lt_res_peak_values_lowg_threshold\n",
    "\n",
    "for key in plot_keys_lt_lowg:\n",
    "    fig = line_plots_lt_lowg.get(key)\n",
    "    if fig is not None:\n",
    "        # If 'peaks_2' trace already exists, remove it before adding new one\n",
    "        fig.data = [trace for trace in fig.data if trace.name != 'peaks']\n",
    "\n",
    "        # Check if key exists in peaks_to_plot\n",
    "        if key in peaks_to_plot:\n",
    "            # Get corresponding DataFrame\n",
    "            df_lowg_peaks = peaks_to_plot[key]\n",
    "            # Add points to figure\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df_lowg_peaks['time_s_scaled'], \n",
    "                y=df_lowg_peaks['peak_values'], \n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color='black',  # for example, choose a color that stands out\n",
    "                ),\n",
    "                name='peaks'  # you can name the trace to be referenced in legend\n",
    "            ))\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(f\"No plot found with key {key}\")\n",
    "\n",
    "# right\n",
    "peaks_to_plot = dfs_rt_res_peak_values_lowg_threshold\n",
    "\n",
    "for key in plot_keys_rt_lowg:\n",
    "    fig = line_plots_rt_lowg.get(key)\n",
    "    if fig is not None:\n",
    "        # If 'peaks_2' trace already exists, remove it before adding new one\n",
    "        fig.data = [trace for trace in fig.data if trace.name != 'peaks']\n",
    "\n",
    "        # Check if key exists in peaks_to_plot\n",
    "        if key in peaks_to_plot:\n",
    "            # Get corresponding DataFrame\n",
    "            df_lowg_peaks = peaks_to_plot[key]\n",
    "            # Add points to figure\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df_lowg_peaks['time_s_scaled'], \n",
    "                y=df_lowg_peaks['peak_values'], \n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color='black',  # for example, choose a color that stands out\n",
    "                ),\n",
    "                name='peaks'  # you can name the trace to be referenced in legend\n",
    "            ))\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(f\"No plot found with key {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highg ---\n",
    "\n",
    "# left\n",
    "peaks_to_plot = dfs_lt_res_peak_values_highg_threshold\n",
    "\n",
    "for key in plot_keys_lt_highg:\n",
    "    fig = line_plots_lt_highg.get(key)\n",
    "    if fig is not None:\n",
    "        # If 'peaks_2' trace already exists, remove it before adding new one\n",
    "        fig.data = [trace for trace in fig.data if trace.name != 'peaks']\n",
    "\n",
    "        # Check if key exists in peaks_to_plot\n",
    "        if key in peaks_to_plot:\n",
    "            # Get corresponding DataFrame\n",
    "            df_highg_peaks = peaks_to_plot[key]\n",
    "            # Add points to figure\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df_highg_peaks['time_s_scaled'], \n",
    "                y=df_highg_peaks['peak_values'], \n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color='black',  # for example, choose a color that stands out\n",
    "                ),\n",
    "                name='peaks'  # you can name the trace to be referenced in legend\n",
    "            ))\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(f\"No plot found with key {key}\")\n",
    "\n",
    "# right\n",
    "peaks_to_plot = dfs_rt_res_peak_values_highg_threshold\n",
    "\n",
    "for key in plot_keys_rt_highg:\n",
    "    fig = line_plots_rt_highg.get(key)\n",
    "    if fig is not None:\n",
    "        # If 'peaks_2' trace already exists, remove it before adding new one\n",
    "        fig.data = [trace for trace in fig.data if trace.name != 'peaks']\n",
    "\n",
    "        # Check if key exists in peaks_to_plot\n",
    "        if key in peaks_to_plot:\n",
    "            # Get corresponding DataFrame\n",
    "            df_highg_peaks = peaks_to_plot[key]\n",
    "            # Add points to figure\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df_highg_peaks['time_s_scaled'], \n",
    "                y=df_highg_peaks['peak_values'], \n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color='black',  # for example, choose a color that stands out\n",
    "                ),\n",
    "                name='peaks'  # you can name the trace to be referenced in legend\n",
    "            ))\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(f\"No plot found with key {key}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Variables to Excel Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add suffix to varible names\n",
    "\n",
    "# highg ---\n",
    "# left\n",
    "lt_res_peak_accel_highg_df['variable'] = lt_res_peak_accel_highg_df['variable'] + '_lt_1600hz'\n",
    "# right\n",
    "rt_res_peak_accel_highg_df['variable'] = rt_res_peak_accel_highg_df['variable'] + '_rt_1600hz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table to export & append rows to to my processed variables table in Excel ---\n",
    "\n",
    "# highg --\n",
    "# left\n",
    "df_to_export = prep.export_tbl(lt_res_peak_accel_highg_df)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)\n",
    "# right\n",
    "df_to_export = prep.export_tbl(rt_res_peak_accel_highg_df)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride Time\n",
    "\n",
    "Calculate Stride Times (STs) using the time between each consecutive footstrikes of single leg  (time of step 2 - time of step 1, time of step 3 - time of step 2, etc.)\n",
    " - Mean\n",
    " - Standard Deviation (SD)\n",
    " - Coefficient of Variance (CV)\n",
    " - Strides per min (SPM)\n",
    " - Fractal Scaling Index (FSI) via Detrended Fluctuation Analysis (DFA)\n",
    "\n",
    "**Note:** Using HighG data for left and right tibia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the time column where each peak occured, calculate the difference between consecutive foot strikes\n",
    "# Creates a new column called stride_times for each run\n",
    "\n",
    "# highg ---\n",
    "# left\n",
    "dfs_lt_stride_times_highg = stride.calc_stride_times(dfs=dfs_lt_res_peak_values_highg_threshold , time_column=\"time_s_scaled\")\n",
    "# right\n",
    "dfs_rt_stride_times_highg = stride.calc_stride_times(dfs=dfs_rt_res_peak_values_highg_threshold , time_column=\"time_s_scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table for stride times\n",
    "\n",
    "k = 3 #IQR \n",
    "z = 3 #SDs\n",
    "\n",
    "# highg ---\n",
    "st_summary_tbl_lt_highg = stats.create_summary_tbl(dfs_lt_stride_times_highg, ['stride_times'], k=k, z=z)\n",
    "st_summary_tbl_rt_highg = stats.create_summary_tbl(dfs_rt_stride_times_highg, ['stride_times'], k=k, z=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stride time outliers based on the upper and lower threshold values created in the summary table above\n",
    "\n",
    "# highg ---\n",
    "# left\n",
    "dfs_lt_stride_times_highg_no_outliers, counts_dfs_lt_stride_times_highg = stats.remove_outliers(\n",
    "    dfs_lt_stride_times_highg, 'stride_times', \n",
    "    st_summary_tbl_lt_highg, id_column='id', lower_threshold_column='lower_bound_k', upper_threshold_column='upper_bound_k')\n",
    "# right\n",
    "dfs_rt_stride_times_highg_no_outliers, counts_dfs_rt_stride_times_highg = stats.remove_outliers(\n",
    "    dfs_rt_stride_times_highg, 'stride_times', \n",
    "    st_summary_tbl_rt_highg, id_column='id', lower_threshold_column='lower_bound_k', upper_threshold_column='upper_bound_k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate stride time variables \n",
    "# Returns a single table with the columns key, variable, and value\n",
    "\n",
    "# highg ---\n",
    "# left\n",
    "st_vars_lt_highg_df = stride.calc_stride_times_vars(dfs_lt_stride_times_highg,'stride_times', total_run_time_mins=5)\n",
    "# right\n",
    "st_vars_rt_highg_df = stride.calc_stride_times_vars(dfs_rt_stride_times_highg, 'stride_times', total_run_time_mins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Variables to Excel Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add suffix to varible names\n",
    "\n",
    "# highg ---\n",
    "# left\n",
    "st_vars_lt_highg_df['variable'] = st_vars_lt_highg_df['variable'] + '_lt_1600hz'\n",
    "# right\n",
    "st_vars_rt_highg_df['variable'] = st_vars_rt_highg_df['variable'] + '_rt_1600hz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table to export & append rows to to my processed variables table in Excel ---\n",
    "\n",
    "# highg --\n",
    "# left\n",
    "df_to_export = prep.export_tbl(st_vars_lt_highg_df)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)\n",
    "# right\n",
    "df_to_export = prep.export_tbl(st_vars_rt_highg_df)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
