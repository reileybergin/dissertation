---
title: "Ch.3"
subtitle: "IMU Validity & Reliability"
date: "`r format(Sys.time(), '%d %B, %Y')`" # today's date
author: "Reiley Bergin"
title-block-banner: "#1B365D"
execute:
  warning: false
  message: false
format: 
  html:
    embed-resources: true
    smooth-scroll: true
    toc: true
    toc-depth: 3
    echo: false
    code-tools: true
    link-external-newwindow: true
editor: visual
---

## Reliability

```{r}
#| label: Packages

library(tidyverse)
library(readr)
library(brms)
library(psych)
library(lme4)
library(easystats)
library(tidybayes)
library(DT)
library(reactable)
```

```{r}
#| label: Data Prep

reliability_data <- read_csv("data/ch.3_reliability_data.csv") %>%
  
  # time pt one = 0 & time pt two = 1
  mutate(
    time_pt = case_when(
      str_detect(run_type, "time01") ~ 0,
      str_detect(run_type, "time02") ~ 1, TRUE ~ NA_real_ )) %>%
  
  mutate(
    run_type = str_extract(run_type, "[^_]+$"),  
    variable = str_replace(variable, "_500hz$", "")) %>%
  
  
  # lb = low back, rt = right tibia, lt = left tibia, 
  mutate(
    variable = str_replace_all(variable, c(
      "accel_x \\(m/s2\\)_meanshift_filtered_rms" = "lb_accel_x_rms_g",
      "accel_y \\(m/s2\\)_meanshift_filtered_rms" = "lb_accel_y_rms_g",
      "accel_z \\(m/s2\\)_meanshift_filtered_rms" = "lb_accel_z_rms_g", 
      "res_g_avg_peak_lt" = "lt_res_pk_accel_g",
      "res_g_avg_peak_rt" = "rt_res_pk_accel_g",
      "res_g_avg_peak_back" = "lb_res_pk_accel_g"
    ))
  )

# Calculate the average for the left and right tibia 
avg_rt_lt <- reliability_data %>%
  filter(variable %in% c("lt_res_pk_accel_g", "rt_res_pk_accel_g")) %>%
  group_by(sub_id, run_type, time_pt) %>%
  summarize(avg_rt_lt_res_pk_accel_g = mean(value, na.rm = TRUE), .groups = 'drop') %>%
  
  # format the table to match the orginal
  mutate(
    variable = "avg_rt_lt_res_pk_accel_g", 
    value = avg_rt_lt_res_pk_accel_g) %>%
  select(sub_id, run_type, variable, value, time_pt)

# Append the new rows to the original dataset
reliability_data <- bind_rows(reliability_data, avg_rt_lt) %>%
  arrange(sub_id, run_type, variable)

# List of Vaiables
vars_reliability <- reliability_data %>%
  distinct(variable) %>%
  arrange(variable) %>%
  mutate(location = case_when(
    str_detect(variable, '^(rt_|lt_|avg_)') ~ 'Tibia',
    str_detect(variable, '^lb_') ~ 'Low Back',
    TRUE ~ NA_character_)) %>%
   select(location, variable) %>%
   arrange(desc(location))
```

```{r}
#| label: Variable Table

reactable(
  vars_reliability,
  striped = TRUE,
  highlight = TRUE,
  bordered = TRUE,
  columns = list(
    location = colDef(name = "IMU Location"),
    variable = colDef(name = "Variable"))
)
```

```{r}

##### Tibia Peak Accel (PA) 

# Running ---

# wide
run_tibia_pa <- read_csv("data/ch.3_reliability_data.csv") %>%
  filter(str_detect(run_type, "_run$")) %>%
  filter(variable == "res_g_avg_peak_rt_500hz") %>%
  select(-variable) %>%
  mutate(run_type = str_replace(run_type, "_.*", "")) %>% #removes _500hz
  pivot_wider(names_from = run_type, values_from = value)

# long (for models)
run_tibia_pa_lg <- run_tibia_pa %>%
  pivot_longer(cols = c(time01, time02), names_to = "time", values_to = "value") %>%
  mutate(time = if_else(time == "time01", 0, 1))

# Walking ---

# wide
walk_tibia_pa <- read_csv("data/ch.3_reliability_data.csv") %>%
  filter(str_detect(run_type, "_walk$")) %>%
  filter(variable == "res_g_avg_peak_rt_500hz") %>%
  select(-variable) %>%
  mutate(run_type = str_replace(run_type, "_.*", "")) %>% #removes _500hz
  pivot_wider(names_from = run_type, values_from = value)

# long (for models)
walk_tibia_pa_lg <- walk_tibia_pa %>%
  pivot_longer(cols = c(time01, time02), names_to = "time", values_to = "value") %>%
  mutate(time = if_else(time == "time01", 0, 1))


##### Lower Back Peak Accel 

```

### Intraclass Correlation Coefficient (ICC)

Using `ICC()` from [psych](https://cran.r-project.org/web/packages/psych/psych.pdf) package and `icc()`from the [easystats](https://github.com/easystats/easystats) package.

#### Frequentist Approach

```{r}

# Looking at ICC3 from psych package
run_tibia_pa %>%
  select(-sub_id) %>%
  ICC()
```

```{r}

# Multilevel Model (varying interceps)
freq_model <- lmer(data = run_tibia_pa_lg, value ~ 1 + time + (1 | sub_id))
```

```{r}

# ICC point estimate
icc_freq <- icc(freq_model)
icc_freq
```

#### Bayesian Approach

```{r}

# Multilevel Model (varying interceps)
bayes_model <- brm(
  
               data = run_tibia_pa_lg,
                   
               family = gaussian(),
               
               value ~ 1 + time + (1 | sub_id),
               
               prior = c(prior(normal(12, 3.0), class = Intercept),
                        prior(normal(0, 2.0), class = b), 
                        prior(cauchy(0, 10), class = sd),
                        prior(cauchy(0, 10), class = sigma)),
               
               warmup = 2000, iter = 10000,
               control = list(adapt_delta = 0.98)
               
               )
```

```{r}

# Draws from Posteior
draws <- bayes_model %>% 
         spread_draws(b_Intercept, b_time, sd_sub_id__Intercept, sigma)
```

```{r}

# ICC point estimate
icc_bayes <- icc(bayes_model)
icc_bayes
```

Estimation of Credible Interval for ICC

```{r}

# Extracting posterior samples
posterior_samples <- as_draws_df(bayes_model, variable = c("sd_sub_id__Intercept", "sigma"))

# Calculate ICC for each posterior sample
icc_distribution <- posterior_samples %>%
  mutate(
    sd_sub_id__Intercept = sd_sub_id__Intercept^2,
    sigma = sigma^2
  ) %>%
  mutate(icc = sd_sub_id__Intercept / (sd_sub_id__Intercept + sigma))
```

```{r}

# Assuming icc_distribution is the data frame containing the ICC values
icc_89_credible_interval <- quantile(icc_distribution$icc, probs = c(0.055, 0.945))
icc_89_credible_interval
```

### Standard Error of Measurement (SEM)

Calculatied as the *mean square error term from a repeated measures ANOVA* as desribed Weir & Vincent in [Statistics in Kinesiology 5th Edition.](https://us.humankinetics.com/blogs/excerpt/standard-error-of-measurement)

#### Frequentist Approach

```{r}

anova_model <- aov(value ~ time + Error(sub_id/time), data = run_tibia_pa_lg)
anova_summary <- summary(anova_model)
print(anova_summary)
```

```{r}

# MSE from the ANOVA summary output
mse <-  1.499

# Calculate the SEM
sem_anova <- sqrt(mse)

print(sem_anova)
```

#### Bayesian Approach

```{r}

# Calculate the 89% credible interval for the SEM using sigma

posterior_samples <- as_draws_df(bayes_model, variable = "sigma") %>%
  as_tibble()

sem_89_credible_interval <- quantile(posterior_samples$sigma, probs = c(0.055, 0.945))

print(sem_89_credible_interval)
```

### Coefficient of Variation (CoV)

#### Frequentist Approach

Calculatied by dividing the SEM (as calculated above) from the grand mean as desribed Weir & Vincent in [Statistics in Kinesiology 5th Edition.](https://us.humankinetics.com/blogs/excerpt/standard-error-of-measurement)

```{r}

grand_mean <- mean(run_tibia_pa_lg$value)

freq_cov <- (sem_anova / grand_mean) *100
freq_cov
```

#### Bayesian Approach

```{r}
# Extract posterior samples for intercept, slope, and sigma
posterior_samples <- as_draws_df(bayes_model, variable = c("b_Intercept", "b_time", "sigma")) %>%
  as_tibble()

# Assuming the intercept is a reasonable approximation of the grand mean
# Calculate the CoV for each posterior sample
cov_distribution <- posterior_samples %>%
  mutate(grand_mean = b_Intercept,
         cov = (sigma / grand_mean) * 100)
```

```{r}

# Calculate the 89% credible interval for CoV
cov_89_credible_interval <- quantile(cov_distribution$cov, probs = c(0.055, 0.945))
print(cov_89_credible_interval)
```

```{r}

# Find the x-value (cov value) where the density is maximum (the mode)
dens <- density(cov_distribution$cov)
estimated_mode <- dens$x[which.max(dens$y)]
print(estimated_mode)
```

## Validity

```{r}

# Packages
library(tidyverse)
library(easystats)
library(readr)
library(BayesFactor)
library(see)
```

```{r}

# Running data ---

run <- read_csv("data/ch.3_validity_data.csv") %>%
  filter(run_type == "run", leg == "right") %>%
  # Subjects that have missing data and need to be removed to balance data
  #filter(!(subject %in% c("Sub03", "Sub10", "Sub12", "Sub13"))) %>%
  select(-run_type, -leg) %>%
  pivot_wider(names_from = variable, values_from = value) %>%
  select(starts_with("imu_"), everything()) %>%
  # Remove '_500hz'
  rename_with(~ gsub("_500hz", "", .), .cols = everything())  


# Rt tibia vs force magnitude variables ---
run_tibia_force_mag <- run %>%
  # Keep columns starting with 'imu_' or ending with '_force_mag'
   select(matches("^imu_.*_rt$|^.*_force_mag$"))

# Walking data ---

```

### Pearson Correlation

Using `correlation` from the [easystats](https://github.com/easystats/easystats) package.

#### Frequentist Approach

```{r}

results_freq <- correlation(run)
results_freq
```

```{r}

# Correlation Matriz
results_freq%>%
  summary(redundant = TRUE) %>%
  plot()
```

```{r}

# Scatter Plot
plot(cor_test(run, "max_ankle_force_mag", "imu_yaxis_g_avg_peak_rt"))
```

#### Bayesian Approach

```{r}

corr_bayes <- correlation(run_tibia_force_mag, bayesian = TRUE)
corr_bayes
```

```{r}

result <- correlationBF(run_tibia_force_mag$imu_yaxis_g_avg_peak_rt, run_tibia_force_mag$max_ankle_force_mag)
describe_posterior(result)
```
