---
title: "Ch.3"
subtitle: "IMU Validity & Reliability"
date: "`r format(Sys.time(), '%d %B, %Y')`" # today's date
author: "Reiley Bergin"
title-block-banner: "#1B365D"
execute:
  warning: false
  message: false
format: 
  html:
    embed-resources: true
    smooth-scroll: true
    toc: true
    toc-depth: 3
    echo: false
    code-tools: true
    link-external-newwindow: true
editor: visual
---

## Reliability

```{r}
#| label: Packages

library(tidyverse)
library(readr)
library(brms)
library(psych)
library(lme4)
library(easystats)
library(tidybayes)
library(reactable)
```

```{r}
#| label: Data Prep

reliability_data <- read_csv("data/ch.3_reliability_data.csv") %>%
  
  # time pt one = 0 & time pt two = 1
  mutate(
    time_pt = case_when(
      str_detect(run_type, "time01") ~ 0,
      str_detect(run_type, "time02") ~ 1, TRUE ~ NA_real_ )) %>%
  
  mutate(
    run_type = str_extract(run_type, "[^_]+$"),  
    variable = str_replace(variable, "_500hz$", "")) %>%
  
  
  # lb = low back, rt = right tibia, lt = left tibia, 
  mutate(
    variable = str_replace_all(variable, c(
      "accel_x \\(m/s2\\)_meanshift_filtered_rms" = "lb_accel_x_rms_g",
      "accel_y \\(m/s2\\)_meanshift_filtered_rms" = "lb_accel_y_rms_g",
      "accel_z \\(m/s2\\)_meanshift_filtered_rms" = "lb_accel_z_rms_g", 
      "res_g_avg_peak_lt" = "lt_res_pk_accel_g",
      "res_g_avg_peak_rt" = "rt_res_pk_accel_g",
      "res_g_avg_peak_back" = "lb_res_pk_accel_g"
    ))
  )

# Calculate the average for the left and right tibia 
rt_lt_avg <- reliability_data %>%
  filter(variable %in% c("lt_res_pk_accel_g", "rt_res_pk_accel_g")) %>%
  group_by(sub_id, run_type, time_pt) %>%
  summarize(rt_lt_avg_res_pk_accel_g = mean(value, na.rm = TRUE), .groups = 'drop') %>%
  
  # format the table to match the orginal
  mutate(
    variable = "rt_lt_avg_res_pk_accel_g", 
    value = rt_lt_avg_res_pk_accel_g) %>%
  select(sub_id, run_type, variable, value, time_pt)

# Append the new rows to the original dataset
reliability_data <- bind_rows(reliability_data, rt_lt_avg) %>%
  arrange(sub_id, run_type, variable)

# List of Vaiables
vars_reliability <- reliability_data %>%
  distinct(variable) %>%
  arrange(variable) %>%
  mutate(location = case_when(
    str_detect(variable, '^(rt_|lt_|avg_)') ~ 'Tibia',
    str_detect(variable, '^lb_') ~ 'Low Back',
    TRUE ~ NA_character_)) %>%
   select(location, variable) %>%
   arrange(desc(location))
```

```{r}
#| label: Functions


# Mean and SD -----

calculate_summary_stats <- function(data) {
  
  # Step 1: Calculate mean and standard deviation for each combination of variable, run_type, and time_pt
  summary_stats <- data %>%
    group_by(variable, run_type, time_pt) %>%
    summarise(mean_value = mean(value, na.rm = TRUE),
              sd_value = sd(value, na.rm = TRUE)) %>%
    ungroup()

  # Step 2: Pivot the table to have mean_time01, mean_time02, sd_time01, and sd_time02 as separate columns
  final_table <- summary_stats %>%
    pivot_wider(names_from = time_pt, 
                values_from = c(mean_value, sd_value)) %>%
    unnest(cols = c(mean_value_0, mean_value_1,
                    sd_value_0, sd_value_1)) %>%
    rename(mean_time01 = mean_value_0, 
           mean_time02 = mean_value_1,
           sd_time01 = sd_value_0,
           sd_time02 = sd_value_1) %>%
    select(variable, run_type, mean_time01, sd_time01, mean_time02, sd_time02) %>%
    mutate_if(is.numeric, round, digits = 2)

  return(final_table)
}

# Bayesian Model -----

fit_bayes_model <- function(data, variable_filter, run_type_filter, intercept_prior, b_prior) {
  
  # Filter the data based on the specified variable and run_type
  filtered_data <- data %>%
    filter(variable == variable_filter, run_type == run_type_filter)
  
  # Extract mean and sd from the provided priors
  intercept_mean <- intercept_prior[1]
  intercept_sd <- intercept_prior[2]
  b_mean <- b_prior[1]
  b_sd <- b_prior[2]
  
  # Define stanvars for intercept and b priors
  stanvars <- stanvar(intercept_mean, name='intercept_mean') +
              stanvar(intercept_sd, name='intercept_sd') +
              stanvar(b_mean, name='b_mean') +
              stanvar(b_sd, name='b_sd')
  
  # Set up priors using the custom parameters
  prs <- c(prior(normal(intercept_mean, intercept_sd), class = "Intercept"),
           prior(normal(b_mean, b_sd), class = "b"),
           prior(cauchy(0, 10), class = "sd"),
           prior(cauchy(0, 10), class = "sigma"))
  
  # Fit the Bayesian model using the filtered data
  bayes_model <- brm(
    formula = value ~ 1 + time_pt + (1 | sub_id),
    data = filtered_data,
    family = gaussian(),
    prior = prs,
    stanvars = stanvars,
    warmup = 2000, iter = 10000,
    control = list(adapt_delta = 0.98), 
    refresh = 0  # Prevent printing of progress
  )
  
  # Create a list to return the model along with inputs for variable nd run_type
  return(list(model = bayes_model, variable = variable_filter, run_type = run_type_filter))
}

# ICC3,1 from psych package

calculate_freq_icc <- function(data) {
  
  # Get unique combinations of variable and run_type
  combinations <- unique(data[c("variable", "run_type")])
  
  # Initialize an empty data frame to store results
  results <- data.frame(variable=character(), run_type=character(), icc=numeric(),
                        lower_bound=numeric(), upper_bound=numeric(), stringsAsFactors=FALSE)
  
  # Loop over each combination
  for (i in 1:nrow(combinations)) {
    variable_filter <- combinations$variable[i]
    run_type_filter <- combinations$run_type[i]
    
    # Filter the data
    filtered_data <- data %>%
      filter(variable == variable_filter, run_type == run_type_filter) %>%
      pivot_wider(names_from = time_pt, values_from = value, names_prefix = "time_pt_") %>%
      select(time_pt_0, time_pt_1) 
    
    # Calculate ICC
    icc_results <- ICC(filtered_data)
    
    # Extract specific ICC values
    icc_values <- icc_results[["results"]][["ICC"]]
    
    # Extract the third ICC value
    third_icc_value <- icc_values[3]
    
    # Extract lower and upper bounds
    lower_bound <- icc_results[["results"]][["lower bound"]][3]
    upper_bound <- icc_results[["results"]][["upper bound"]][3]
    
    # Create a result row
    result_row <- data.frame(variable = variable_filter,
                             run_type = run_type_filter,
                             icc_freq = third_icc_value,
                             icc_freq_025 = lower_bound,
                             icc_freq_975 = upper_bound)
    
    # Bind this row to the results data frame
    results <- rbind(results, result_row)
  }
  
  return(results)
}


# Calculate ICC estimates using bayesian posterior

calculate_bayes_icc <- function(models_and_metadata_list) {
  
  # Function to process a single model_and_metadata
  process_single_model <- function(model_and_metadata) {
    # Get model
    bmod <- model_and_metadata$model

    # Extracting posterior samples
    posterior_samples <- as_draws_df(bmod, variable = c("sd_sub_id__Intercept", "sigma"))

    # Calculate ICC for each posterior sample
    icc_distribution <- posterior_samples %>%
      mutate(
        sd_intercepts = sd_sub_id__Intercept^2,
        sd_residual = sigma^2 
      ) %>%
      mutate(icc = sd_intercepts / (sd_intercepts + sd_residual))
    
    # Calculate summary statistics
    easystats_icc <- icc(bmod)
    mean_icc <- mean(icc_distribution$icc)
    ci_025 <- quantile(icc_distribution$icc, probs = 0.025)
    ci_975 <- quantile(icc_distribution$icc, probs = 0.975)
    ci_055 <- quantile(icc_distribution$icc, probs = 0.055)
    ci_945 <- quantile(icc_distribution$icc, probs = 0.945)
      
    # Create a single result row
    result_row <- data.frame(variable = model_and_metadata$variable,
                             run_type = model_and_metadata$run_type,
                             icc_bayes_easystats = easystats_icc$ICC_adjusted,
                             icc_bayes_mean = mean_icc,
                             icc_bayes_025 = ci_025,
                             icc_bayes_975 = ci_975, 
                             icc_bayes_055 = ci_055, 
                             icc_bayes_945 = ci_945)

    return(result_row)
  }

  # Apply the function to each element in the list and combine results
  results <- do.call(rbind, lapply(models_and_metadata_list, process_single_model))

  return(results)
}
```

### Summary Statistics

```{r}
# table with mean and sd
stats_tbl <- calculate_summary_stats(reliability_data)
```

```{r}
#| label: Mean & SD Table

reactable(
  stats_tbl,
  striped = TRUE,
  highlight = TRUE,
  bordered = TRUE, 
  filterable = TRUE, 
  pagination = FALSE
)
```

### Bayesian Model

**Model (using peak tibial acceleration as an example):**

$$
\begin{align*}
\text{PTA}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i & = \alpha_{\text{subject}[i]} + \beta_{} \times \text{Time2}_i \\
\alpha        & \sim \operatorname{Normal}(8, 3.0) \\
\beta         & \sim \operatorname{Normal}(0.1, 0.05) \\
\sigma        & \sim \operatorname{HalfCauchy}(10) \\
\sigma_\alpha & \sim \operatorname{HalfCauchy}(10) \\
\end{align*}
$$

```{r}
#| label: Bayes Models

# Running ---

# Tibia Peak Accel

# Right 
run_rt_pta <- fit_bayes_model(reliability_data, "rt_res_pk_accel_g", "run", c(8, 3), c(0.1, 0.05))

# Left 
run_lt_pta <- fit_bayes_model(reliability_data, "lt_res_pk_accel_g", "run", c(8, 3), c(0.1, 0.05))

# Avgerage 
run_avg_pta <- fit_bayes_model(reliability_data, "rt_lt_avg_res_pk_accel_g", "run", c(8, 3), c(0.1, 0.05))
```

### Intraclass Correlation Coefficient (ICC)

Using `ICC()` from [psych](https://cran.r-project.org/web/packages/psych/psych.pdf) package and `icc()`from the [easystats](https://github.com/easystats/easystats) package.

#### Frequentist Approach

```{r}
# Looking at ICC3 from psych package
freq_icc_tbl <- calculate_freq_icc(reliability_data)
```

#### Bayesian Approach

```{r}
# Calculates posterior distribution and samples ICC values
bayes_icc_tbl <- calculate_bayes_icc(list(run_avg_pta, run_rt_pta, run_lt_pta))
```

### Standard Error of Measurement (SEM)

Calculatied as the *mean square error term from a repeated measures ANOVA* as desribed Weir & Vincent in [Statistics in Kinesiology 5th Edition.](https://us.humankinetics.com/blogs/excerpt/standard-error-of-measurement)

#### Frequentist Approach

```{r}

data <- reliability_data %>%
    filter(variable == "rt_res_pk_accel_g", run_type == "run")

```

```{r}

anova_model <- aov(value ~ time_pt + Error(sub_id/time_pt), data = data)
anova_summary <- summary(anova_model)
print(anova_summary)
```

```{r}

# MSE from the ANOVA summary output
mse <-  1.499

# Calculate the SEM
sem_anova <- sqrt(mse)

print(sem_anova)
```

#### Bayesian Approach

```{r}

# Calculate the 89% credible interval for the SEM using sigma

posterior_samples <- as_draws_df(bayes_model, variable = "sigma") %>%
  as_tibble()

sem_89_credible_interval <- quantile(posterior_samples$sigma, probs = c(0.055, 0.945))

print(sem_89_credible_interval)
```

### Coefficient of Variation (CoV)

#### Frequentist Approach

Calculatied by dividing the SEM (as calculated above) from the grand mean as desribed Weir & Vincent in [Statistics in Kinesiology 5th Edition.](https://us.humankinetics.com/blogs/excerpt/standard-error-of-measurement)

```{r}

grand_mean <- mean(run_tibia_pa_lg$value)

freq_cov <- (sem_anova / grand_mean) *100
freq_cov
```

#### Bayesian Approach

```{r}
# Extract posterior samples for intercept, slope, and sigma
posterior_samples <- as_draws_df(bayes_model, variable = c("b_Intercept", "b_time", "sigma")) %>%
  as_tibble()

# Assuming the intercept is a reasonable approximation of the grand mean
# Calculate the CoV for each posterior sample
cov_distribution <- posterior_samples %>%
  mutate(grand_mean = b_Intercept,
         cov = (sigma / grand_mean) * 100)
```

```{r}

# Calculate the 89% credible interval for CoV
cov_89_credible_interval <- quantile(cov_distribution$cov, probs = c(0.055, 0.945))
print(cov_89_credible_interval)
```

```{r}

# Find the x-value (cov value) where the density is maximum (the mode)
dens <- density(cov_distribution$cov)
estimated_mode <- dens$x[which.max(dens$y)]
print(estimated_mode)
```

## Validity

```{r}

# Packages
library(tidyverse)
library(easystats)
library(readr)
library(BayesFactor)
library(see)
```

```{r}

# Running data ---

run <- read_csv("data/ch.3_validity_data.csv") %>%
  filter(run_type == "run", leg == "right") %>%
  # Subjects that have missing data and need to be removed to balance data
  #filter(!(subject %in% c("Sub03", "Sub10", "Sub12", "Sub13"))) %>%
  select(-run_type, -leg) %>%
  pivot_wider(names_from = variable, values_from = value) %>%
  select(starts_with("imu_"), everything()) %>%
  # Remove '_500hz'
  rename_with(~ gsub("_500hz", "", .), .cols = everything())  


# Rt tibia vs force magnitude variables ---
run_tibia_force_mag <- run %>%
  # Keep columns starting with 'imu_' or ending with '_force_mag'
   select(matches("^imu_.*_rt$|^.*_force_mag$"))

# Walking data ---

```

### Pearson Correlation

Using `correlation` from the [easystats](https://github.com/easystats/easystats) package.

#### Frequentist Approach

```{r}

results_freq <- correlation(run)
results_freq
```

```{r}

# Correlation Matriz
results_freq%>%
  summary(redundant = TRUE) %>%
  plot()
```

```{r}

# Scatter Plot
plot(cor_test(run, "max_ankle_force_mag", "imu_yaxis_g_avg_peak_rt"))
```

#### Bayesian Approach

```{r}

corr_bayes <- correlation(run_tibia_force_mag, bayesian = TRUE)
corr_bayes
```

```{r}

result <- correlationBF(run_tibia_force_mag$imu_yaxis_g_avg_peak_rt, run_tibia_force_mag$max_ankle_force_mag)
describe_posterior(result)
```
