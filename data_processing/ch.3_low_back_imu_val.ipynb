{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMU Validation Study Low Back Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMeasureU Blue Thunder Specs:\n",
    "- Sampling freq for lowg = 500hz\n",
    "- 5 minutes of data = 337,500 rows (1125hz) or 478,000 (1600hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom functions ---\n",
    "import functions.file_import_gui as gui\n",
    "import functions.data_prep as prep\n",
    "import functions.custom_plots as plots\n",
    "import functions.low_back_measures as back\n",
    "import functions.peak_detection as peaks\n",
    "import functions.stats as stats\n",
    "\n",
    "# For saving files\n",
    "import os\n",
    "\n",
    "# For dataframes ---\n",
    "import pandas as pd\n",
    "\n",
    "# For plotting ---\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject to process\n",
    "sub_id = 'imu_val_002'\n",
    "time_pt = 'time2'\n",
    "run_type = 'run'\n",
    "\n",
    "# low g (500hz) ---\n",
    "# set directory\n",
    "initialdir = f\"data/five_min_runs/imu_validation_study/{sub_id}/{time_pt}/low_back/{run_type}\"\n",
    "# bring in csv files with data\n",
    "dfs_lowg, keys_list = gui.read_csv_files_gui(initialdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep ---\n",
    "\n",
    "# crop data for 5 mins (removes extra rows at the beginning so that there are exactly 5 mins of data)\n",
    "prep.crop_df_five_mins(dfs_lowg, sample_freq = 500)\n",
    "# calculate and add resultant column\n",
    "prep.add_resultant_column(dfs_lowg, column_x = 'accel_x (m/s2)', column_y = 'accel_y (m/s2)', column_z = 'accel_z (m/s2)', name_of_res_column = 'res_m/s/s')\n",
    "# convert accel columns to gs\n",
    "prep.accel_to_gs_columns(dfs_lowg, column_x = 'accel_x (m/s2)', column_y = 'accel_y (m/s2)', column_z = 'accel_z (m/s2)', name_of_res_column = 'res_m/s/s')\n",
    "# shift time scale to start at 0\n",
    "prep.shift_time_s_to_zero(dfs_lowg, time_col='timestamp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meanshift ---\n",
    "prep.calc_mean_shift(dfs_lowg, ['accel_x (m/s2)', 'accel_y (m/s2)', 'accel_z (m/s2)', 'ax_g', 'ay_g', 'az_g'])\n",
    "# calculate resultant from meanshifted signals\n",
    "prep.add_resultant_column(\n",
    "    dfs_lowg, column_x = 'accel_x (m/s2)_meanshift', column_y = 'accel_y (m/s2)_meanshift', \n",
    "    column_z = 'accel_z (m/s2)_meanshift', name_of_res_column = 'res_m/s/s_meanshift'\n",
    "    )\n",
    "prep.add_resultant_column(\n",
    "    dfs_lowg, column_x = 'ax_g_meanshift', column_y = 'ay_g_meanshift', \n",
    "    column_z = 'az_g_meanshift', name_of_res_column = 'res_g_meanshift'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data ---\n",
    "\n",
    "cutoff_frequency = 50 #hz\n",
    "filter_order = 4 #th\n",
    "\n",
    "# lowg ---\n",
    "sampling_frequency = 500 #hz \n",
    "# Filter raw data\n",
    "columns_to_filter = [\n",
    "    'accel_x (m/s2)', 'accel_y (m/s2)', 'accel_z (m/s2)', 'res_m/s/s', \n",
    "    'ax_g', 'ay_g', 'az_g', 'res_g']\n",
    "prep.apply_butter_lowpass_filter_to_dfs(dfs_lowg, columns_to_filter, sampling_frequency, cutoff_frequency, filter_order)\n",
    "# Filter mean shift data\n",
    "columns_to_filter = [\n",
    "    'accel_x (m/s2)_meanshift', 'accel_y (m/s2)_meanshift', 'accel_z (m/s2)_meanshift', 'res_m/s/s_meanshift', \n",
    "    'ax_g_meanshift', 'ay_g_meanshift', 'az_g_meanshift', 'res_g_meanshift']\n",
    "prep.apply_butter_lowpass_filter_to_dfs(dfs_lowg, columns_to_filter, sampling_frequency, cutoff_frequency, filter_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Center of Mass (CoM) Measures\n",
    "- **Root Mean Squared (RMS)** - a single value for each axis of acceleration (m/s/s) VT, ML, AP, resultant (RES)\n",
    "- **RMS Ratio** - the RMS of each axis is divided by the resultant root mean squared, for example VT_RMS/RES_RMS\n",
    "- **AVG Peak Acceleration of Resultant** - finds peaks of resultant and calculates the avg of these peaks\n",
    "\n",
    "**NOTE:** \n",
    "- The RMS and RMS Ratio are *mean-shifted* and calculated from the *filtered* signal\n",
    "- AVG Peak Accel uses the *raw* signal (no mean-shift or filtering)\n",
    "\n",
    "Axis Orientation:\n",
    "- **X-axis**: represents the **medial-lateral (ML)** direction, with positive values pointing to the right and negative values pointing to the left. This corresponds to the side-to-side movement of the body.\n",
    "- **Y-axis**: aligned with the **vertical (VT)** direction, with positive values indicating a superior (upward) direction and negative values indicating an inferior (downward) direction. This corresponds to the up and down movement of the body.\n",
    "- **Z-axis**: oriented in the **anterior-posterior (AP)** direction, with positive values pointing anterior (forward) and negative values pointing posterior (backward). This represents the forward and backward movement of the body."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS Calculations ---\n",
    "\n",
    "columns_for_rms = [\n",
    "    'accel_x (m/s2)', 'accel_y (m/s2)', 'accel_z (m/s2)', 'res_m/s/s', # raw\n",
    "    'accel_x (m/s2)_filtered', 'accel_y (m/s2)_filtered', 'accel_z (m/s2)_filtered', 'res_m/s/s_filtered', # filtered\n",
    "    'accel_x (m/s2)_meanshift_filtered', 'accel_y (m/s2)_meanshift_filtered', 'accel_z (m/s2)_meanshift_filtered', 'res_m/s/s_meanshift_filtered' # mean shifted & filtered\n",
    "    ]\n",
    "# Use custom function (back.apply_rms_to_dfs)\n",
    "# returns a table in long format (variables are in a column and the values are in another)\n",
    "# also adds suffix at end of each value in the column variable (need this later to know which hz)\n",
    "rms_df_lowg = back.apply_rms_to_dfs(dfs_lowg, columns_for_rms)\n",
    "rms_df_lowg['variable'] = rms_df_lowg['variable'] + '_500hz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table to export & append rows to to my processed variables table in Excel ---\n",
    "\n",
    "df_to_export = prep.export_tbl_imu_val(rms_df_lowg)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMS Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS Ratio Calculations ---\n",
    "\n",
    "# filtered signal ---\n",
    "# pivot the rms df so its easier to work with\n",
    "pivot_rms_df_lowg_wide = rms_df_lowg.pivot(index='key', columns='variable', values='value')\n",
    "# calculate the ratios for each column (variable)\n",
    "for axis in ['accel_x (m/s2)_filtered_rms_500hz', 'accel_y (m/s2)_filtered_rms_500hz', 'accel_z (m/s2)_filtered_rms_500hz']:\n",
    "    pivot_rms_df_lowg_wide[axis+'_ratio'] = pivot_rms_df_lowg_wide[axis] / pivot_rms_df_lowg_wide['res_m/s/s_filtered_rms_500hz']\n",
    "# keep only the ratio columns\n",
    "pivot_rms_df_lowg_wide = pivot_rms_df_lowg_wide[[col for col in pivot_rms_df_lowg_wide.columns if 'ratio' in col]]\n",
    "# melt the df back to a long format\n",
    "rms_filtered_ratio_df_lowg = pivot_rms_df_lowg_wide.reset_index().melt(id_vars='key', var_name='variable', value_name='value')\n",
    "\n",
    "# mean shifted filtered signal ---\n",
    "# pivot the rms df so its easier to work with\n",
    "pivot_rms_df_lowg_wide = rms_df_lowg.pivot(index='key', columns='variable', values='value')\n",
    "# calculate the ratios for each column (variable)\n",
    "for axis in ['accel_x (m/s2)_meanshift_filtered_rms_500hz', 'accel_y (m/s2)_meanshift_filtered_rms_500hz', 'accel_z (m/s2)_meanshift_filtered_rms_500hz']:\n",
    "    pivot_rms_df_lowg_wide[axis+'_ratio'] = pivot_rms_df_lowg_wide[axis] / pivot_rms_df_lowg_wide['res_m/s/s_meanshift_filtered_rms_500hz']\n",
    "# keep only the ratio columns\n",
    "pivot_rms_df_lowg_wide = pivot_rms_df_lowg_wide[[col for col in pivot_rms_df_lowg_wide.columns if 'ratio' in col]]\n",
    "# melt the df back to a long format\n",
    "rms_meanshift_filtered_ratio_df_lowg = pivot_rms_df_lowg_wide.reset_index().melt(id_vars='key', var_name='variable', value_name='value')\n",
    "\n",
    "# combine tables above\n",
    "rms_ratio_df_lowg = pd.concat([rms_filtered_ratio_df_lowg, rms_meanshift_filtered_ratio_df_lowg], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table to export & append rows to to my processed variables table in Excel ---\n",
    "\n",
    "df_to_export = prep.export_tbl_imu_val(rms_ratio_df_lowg)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AVG Peak Acceleration of Resultant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for functions below\n",
    "# The minimum time between peaks i.e. footstrikes corresponding to 0.25 secs for running and 0.35 secs for walking\n",
    "\n",
    "if run_type == \"run\":\n",
    "    lowg_time_between_peaks = 125\n",
    "elif run_type == \"walk\":\n",
    "    lowg_time_between_peaks = 175\n",
    "else:\n",
    "    raise ValueError(\"Invalid run_type. Please set run_type to 'run' or 'walk'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1:\n",
    "\n",
    "# Find peaks with no thresholds\n",
    "# NOTE: This will be used to then determine individual thresholds for max peak height\n",
    "\n",
    "_, dfs_res_peak_values_lowg_no_threshold = peaks.calc_avg_positive_peaks(\n",
    "    dfs_lowg, columns=['res_g'], time_column='time_s_scaled', \n",
    "    min_peak_height=None, max_peak_height=None,\n",
    "    min_samples_between_peaks=lowg_time_between_peaks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: \n",
    "\n",
    "# Determine subject's individual thresholds for max_peak_height and min_peak_height from peaks indentified w/ no threshold\n",
    "\n",
    "k = 4 #IQR \n",
    "z = 4 #SDs\n",
    "\n",
    "# summary table w/upper bound\n",
    "summary_tbl_lowg = stats.create_summary_tbl(dfs_res_peak_values_lowg_no_threshold, ['peak_values'], k=k, z=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3:\n",
    "\n",
    "# Use individualized max and min peak height threshold as upper limit for finding peaks\n",
    "# NOTE: This uses a different peak function that takes the summary table as inputs and steps through the indiv rows of each run/sensor\n",
    "\n",
    "res_peak_accel_lowg_df, dfs_res_peak_values_lowg_threshold = peaks.calc_avg_positive_peaks_from_tbl(\n",
    "    dfs_lowg, ['res_g'], time_column='time_s_scaled',\n",
    "    summary_table=summary_tbl_lowg, id_column=\"id\", min_peak_height_column=\"lower_bound_k\", max_peak_height_column=\"upper_bound_k\",\n",
    "    min_samples_between_peaks=lowg_time_between_peaks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually Inspect Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot keys for specific runs to visualize below ---\n",
    "\n",
    "# Grab all keys from the dictionaries\n",
    "dfs_lowg_plots = {key: value for key, value in dfs_lowg.items()}\n",
    "\n",
    "# Create and store plots for specified columns ---\n",
    "\n",
    "x_col = 'time_s_scaled'\n",
    "y_cols = ['res_g']\n",
    "\n",
    "line_plots_lowg = plots.create_line_plots(dfs_lowg_plots, x_col, y_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_to_plot = dfs_res_peak_values_lowg_threshold\n",
    "\n",
    "# Iterate over the keys in dfs_lowg\n",
    "for key in dfs_lowg:\n",
    "    fig = line_plots_lowg.get(key)\n",
    "    if fig is not None:\n",
    "        # If 'peaks' trace already exists, remove it before adding new one\n",
    "        fig.data = [trace for trace in fig.data if trace.name != 'peaks']\n",
    "\n",
    "        # Check if key exists in peaks_to_plot\n",
    "        if key in peaks_to_plot:\n",
    "            # Get corresponding DataFrame\n",
    "            df_lowg_peaks = peaks_to_plot[key]\n",
    "            # Add points to figure\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df_lowg_peaks['time_s_scaled'], \n",
    "                y=df_lowg_peaks['peak_values'], \n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color='black',  # for example, choose a color that stands out\n",
    "                ),\n",
    "                name='peaks'  # you can name the trace to be referenced in legend\n",
    "            ))\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(f\"No plot found with key {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add suffixes to variables\n",
    "\n",
    "# lowg ---\n",
    "res_peak_accel_lowg_df['variable'] = res_peak_accel_lowg_df['variable'] + '_back_500hz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table to export & append rows to to my processed variables table in Excel ---\n",
    "\n",
    "df_to_export = prep.export_tbl_imu_val(res_peak_accel_lowg_df)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imu_training_load_study-UQqD6JkG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
