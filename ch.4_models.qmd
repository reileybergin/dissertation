---
title: "Ch.4 Models"
subtitle: ""
date: "`r format(Sys.time(), '%d %B, %Y')`" # today's date
author: "Reiley Bergin"
title-block-banner: "#1B365D"
execute:
  warning: false
  message: false
format: 
  html:
    embed-resources: true
    smooth-scroll: true
    toc: true
    toc-depth: 3
    echo: false
    code-tools: true
    link-external-newwindow: true
editor: visual
---

```{r}
library(tidyverse)
library(brms)
```

```{r}
# real data
data <- reliability %>%
  pivot_longer(cols = c(Time1, Time2), names_to = "time", values_to = "value") %>%
  mutate(time = if_else(time == "Time1", 0, 1))
```

### Model

$$
\begin{align*}
\text{PTA}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i         & = \alpha_{\text{subject}[i]} + \beta_{\text{subject}[i]} \text{Heavy}_i \\
\begin{bmatrix} \alpha_\text{subject} \\ \beta_\text{subject} \end{bmatrix} & \sim \operatorname{MVNormal} \begin{pmatrix} \begin{bmatrix} \alpha \\ \beta \end{bmatrix}, \mathbf \Sigma \end{pmatrix} \\
\mathbf \Sigma     & = \begin{bmatrix} \sigma_\alpha & 0 \\ 0 & \sigma_\beta \end{bmatrix} \mathbf R \begin{bmatrix} \sigma_\alpha & 0 \\ 0 & \sigma_\beta \end{bmatrix} \\
\alpha        & \sim \operatorname{Normal}(12, 3.8) \\
\beta         & \sim \operatorname{Normal}(0, 1.5) \\
\sigma        & \sim \operatorname{Exponential}(1) \\
\sigma_\alpha & \sim \operatorname{Exponential}(1) \\
\sigma_\beta  & \sim \operatorname{Exponential}(1) \\
\mathbf R     & \sim \operatorname{LKJcorr}(2)
\end{align*}
$$

### Prior Predective Simulations

```{r}

# Function to calculate the range for a given normal distribution
calculate_range <- function(mean, std_dev) {
  # Z-scores for the lower and upper bounds (1% and 99%)
  z_score_lower <- qnorm(0.01)
  z_score_upper <- qnorm(0.99)
  
  # Calculate the range of values within the 98% interval
  interval_lower <- mean + z_score_lower * std_dev
  interval_upper <- mean + z_score_upper * std_dev
  
  return(c(interval_lower, interval_upper))
}

# 98% of values fall within
mean <- 14
std_dev <- 4
range <- calculate_range(mean, std_dev)
```

```{r}

a       <-  12  # average morning wait time
b       <- 0    # average difference afternoon wait time
sigma_a <-  4.0    # std dev in intercepts
sigma_b <-  1.5  # std dev in slopes
rho     <- -.2   # correlation between intercepts and slopes

# the next three lines of code simply combine the terms, above
mu     <- c(a, b)

cov_ab <- sigma_a * sigma_b * rho
sigma  <- matrix(c(sigma_a^2, cov_ab, 
                   cov_ab, sigma_b^2), ncol = 2)

sigmas <- c(sigma_a, sigma_b)          # standard deviations
rho    <- matrix(c(1, rho,             # correlation matrix
                   rho, 1), nrow = 2)

# now matrix multiply to get covariance matrix
sigma <- diag(sigmas) %*% rho %*% diag(sigmas)

# how many subjects would you like?
n_subjects <- 10

set.seed(5)  # used to replicate example

vary_effects <- 
  MASS::mvrnorm(n_subjects, mu, sigma) %>% 
  data.frame() %>% 
  set_names("a_subject", "b_subject")

```

```{r}
# Calculate the correlation
correlation <- cor(vary_effects$a_subject, vary_effects$b_subject)

# plot intercepts vs slopes
ggplot(vary_effects, aes(x = a_subject, y = b_subject)) +
  geom_point(color = "#80A0C7") +
  geom_rug(color = "#8B9DAF", linewidth = 1/7) +
  ggtitle(paste("Correlation between intercepts and slopes:", round(correlation, 2)))
```

### Simulate observations

```{r}
n_visits <- 2
sigma    <-  1.5  # std dev within cafes

set.seed(22)  # used to replicate example

d <-
  vary_effects %>% 
  mutate(subject = 1:n_subjects) %>% 
  expand_grid(visit = 1:n_visits) %>% 
  mutate(heavy = rep(0:1, times = n() / 2)) %>% # 1 or 0, half the visits
  mutate(mu = a_subject + b_subject * heavy) %>% 
  mutate(pta = rnorm(n = n(), mean = mu, sd = sigma)) %>% # actual values
  select(subject, everything())
```

### The varying slopes model

```{r}
 bmod_ml <- 
  brm(data = data,
      family = gaussian(),
      
      value ~ 1 + time + (1 + time | Subject),
      
      prior = c(prior(normal(70, 20.0), class = Intercept),
                prior(normal(0, 2.0), class = b),
                prior(cauchy(0, 10), class = sd),
                prior(cauchy(0, 10), class = sigma),
                prior(lkj(2), class = cor)),
      warmup = 2000, iter = 10000)
```

```{r}
 bmod <- 
  brm(data = data,
      family = gaussian(),
      
      value ~ 1 + time,
      
      prior = c(prior(normal(70, 20.0), class = Intercept),
                prior(normal(0, 2.0), class = b)),
      warmup = 2000, iter = 10000)
```

```{r}
bmod_vi <- 
  brm(data = data,
      family = gaussian(),
      
      value ~ 1 + time + (1 | Subject),
      
      prior = c(prior(normal(70, 20.0), class = Intercept),
                prior(normal(0, 2.0), class = b)),
      warmup = 2000, iter = 10000)

```

```{r}
coef(bmod_vi)
```

```{r}
post <- as_draws_df(bmod_vi)
```

```{r}
post %>%
  ggplot() +
  geom_density(aes(x = cor_cafe__Intercept__afternoon),
               color = "transparent", fill = "#A65141", alpha = 9/10) +
  annotate(geom = "text", 
           x = c(-0.15, 0), y = c(2.21, 0.85), 
           label = c("posterior", "prior"), 
           color = c("#A65141", "#EEDA9D")) +
  scale_y_continuous(NULL, breaks = NULL) 
```

```{r}
coef(b14.2)
```

```{r}
partially_pooled_params <-
  # with this line we select each of the 20 cafe's posterior mean (i.e., Estimate)
  # for both `Intercept` and `afternoon`
  coef(b14.1)$cafe[ , 1, 1:2] %>%
  data.frame() %>%              # convert the two vectors to a data frame
  rename(Slope = afternoon) %>%
  mutate(cafe = 1:nrow(.)) %>%  # add the `cafe` index
  select(cafe, everything())    # simply moving `cafe` to the leftmost position
```

```{r}
partially_pooled_estimates <-
  coef(b14.1)$cafe[ , 1, 1:2] %>%
  # convert the two vectors to a data frame
  data.frame() %>%
  # the Intercept is the wait time for morning (i.e., `afternoon == 0`)
  rename(morning = Intercept) %>%
  # `afternoon` wait time is the `morning` wait time plus the afternoon slope
  mutate(afternoon = morning + afternoon,
         cafe      = 1:n()) %>%  # add the `cafe` index
  select(cafe, everything()) 
```

```{r}
posterior_summary(bmod_vi) %>%
  round(digits = 2)
```

```{r}
#summary
summary(b2.5, prob = .89)
```

```{r}
plot_pp_check <- pp_check(b14.1)
plot_pp_check 
```

```{r}
# shiny app
launch_shinystan(b14.1)
```

### Prediction intervals for actual values

```{r}

predictions <-
  predict(b14.1,
          newdata = d, summary = FALSE)


predictions_summary <-
  predict(b14.1,
          newdata = d, probs = c(.055, .945))
```

```{r}
# Assuming V1 is for the morning and V2 for the afternoon of Cafe 1

predictions_tibble <- as_tibble(predictions)

morning_wait_times <- predictions_tibble$V1
afternoon_wait_times <- predictions_tibble$V2
```

```{r}
# contrast
contrast_cafe1 <- afternoon_wait_times - morning_wait_times
```

```{r}
summary_stats <- data.frame(
  Mean = mean(contrast_cafe1),
  Median = median(contrast_cafe1),
  Quantile_5_5 = quantile(contrast_cafe1, 0.055),
  Quantile_94_5 = quantile(contrast_cafe1, 0.945)
)

```

```{r}
long_data <- data.frame(
  Time = factor(rep(c("Morning", "Afternoon"), each = length(morning_wait_times))),
  Wait_Times = c(morning_wait_times, afternoon_wait_times)
)

library(ggplot2)

ggplot(long_data, aes(x = Wait_Times, fill = Time)) +
  geom_density(alpha = 0.5) +
  labs(title = "Distribution of Morning and Afternoon Wait Times for Cafe 1",
       x = "Wait Time",
       y = "Density") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal()
```

```{r}
contrast_data <- data.frame(Contrast = contrast_cafe1)

ggplot(contrast_data, aes(x = Contrast)) +
  geom_density(fill = "blue", alpha = 0.5) +
  labs(title = "Distribution of Contrast (Afternoon - Morning) for Cafe 1",
       x = "Contrast in Wait Time",
       y = "Density") +
  theme_minimal()
```

### ROPE

```{r}
rope_range <- c(-1.5, 1.5)
# Calculate the percentage of values outside the ROPE
outside_rope <- sum(contrast_cafe1 < rope_range[1] | contrast_cafe1 > rope_range[2]) / length(contrast_cafe1)
```

```{r}
ggplot(contrast_data, aes(x = Contrast)) +
  geom_density(fill = "blue", alpha = 0.5) +
  geom_vline(xintercept = rope_range, color = "red", linetype = "dashed") +
  labs(title = "Distribution of Contrast with ROPE for Cafe 1",
       subtitle = paste("Percentage outside ROPE:", round(outside_rope*100, 2), "%"),
       x = "Contrast in Wait Time",
       y = "Density") +
  theme_minimal()

```

### Group Effect Size

```{r}
delta_t <- 
  # Extracting posterior samples from your model using as_draws
  as_draws_df(b14.1, variable = c("b_afternoon", "sd_cafe__Intercept", "sd_cafe__afternoon", "sigma")) %>%
  
  # Taking the square of each variance component
  mutate(
    sd_cafe__Intercept = sd_cafe__Intercept^2,
    sd_cafe__afternoon = sd_cafe__afternoon^2,
    sigma = sigma^2
  ) %>%
  
  # Dividing the slope estimate by the square root of the sum of all variance components
  mutate(delta_t = b_afternoon / sqrt(sd_cafe__Intercept + sd_cafe__afternoon + sigma))

```

```{r}

ggplot(delta_t, aes(x = delta_t)) +
  geom_density(fill = "blue", alpha = 0.5) +
  labs(title = "Density Plot of Standardized Effect Size (\u03B4t)",
       x = "Standardized Effect Size Delta",
       y = "Density") +
  theme_minimal()

```

```{r}
rope_range_es <- c(-0.6, 0.6)
outside_rope_es <- sum(delta_t$delta_t < rope_range_es[1] | delta_t$delta_t > rope_range_es[2]) / nrow(delta_t)
```

```{r}
ggplot(delta_t, aes(x = delta_t)) +
  geom_density(fill = "blue", alpha = 0.5) +
  geom_vline(xintercept = rope_range_es, color = "red", linetype = "dashed") +
  labs(title = "Distribution of Standardized Effect Size (\u03B4t) with ROPE",
       subtitle = paste("Percentage outside ROPE:", round(outside_rope_es*100, 2), "%"),
       x = "Standardized Effect Size (\u03B4t)",
       y = "Density") +
  theme_minimal()
```

## ICC

```{r}
library(performance)

# Assuming bmod is your fitted model
icc_value <- icc(bmod_vi)
print(icc_value)
```

```{r}

# Extracting posterior samples
posterior_samples <- as_draws_df(bmod_vi, variable = c("sd_Subject__Intercept", "sigma"))

# Calculating ICC for each posterior sample
icc_distribution <- posterior_samples %>%
  mutate(
    sd_Subject__Intercept = sd_Subject__Intercept^2,
    sigma = sigma^2
  ) %>%
  mutate(icc = sd_Subject__Intercept / (sd_Subject__Intercept + sigma))
```

```{r}
# Now you can summarize or plot this distribution
summary(icc_distribution$icc)
```

```{r}
# Assuming icc_distribution is the data frame containing the ICC values
icc_89_credible_interval <- quantile(icc_distribution$icc, probs = c(0.055, 0.945))

print(icc_89_credible_interval)
```

SEM

```{r}

# Assuming bmod is your fitted brm model
posterior_samples <- as_draws_df(bmod_vi, variable = "sigma")

# Calculating the SEM as the mean of the posterior samples of the residual standard deviation
sem_mean <- mean(posterior_samples$sigma)
print(sem)

sem_89_credible_interval <- quantile(cov_distribution$cov, probs = c(0.055, 0.945))
```

```{r}

# Assuming bmod_vi is your fitted brm model
posterior_samples <- as_draws_df(bmod_vi, variable = "sigma") %>%
  as_tibble()

# Calculate the 89% credible interval for the SEM
sem_89_credible_interval <- quantile(posterior_samples$sigma, probs = c(0.055, 0.945))

print(sem_89_credible_interval)
```

```{r}
# Fit the repeated measures ANOVA
anova_model <- aov(value ~ time + Error(Subject/time), data = data)

# Print the summary to understand its structure
anova_summary <- summary(anova_model)
print(anova_summary)
```

```{r}
# MSE from the ANOVA summary output
mse <- 19.093

# Calculate the SEM
sem_anova <- sqrt(mse)

print(sem_anova)
```

CoV

```{r}

# Extract posterior samples for intercept, slope, and sigma
posterior_samples <- as_draws_df(bmod_vi, variable = c("b_Intercept", "b_time", "sigma")) %>%
  as_tibble()

# Assuming the intercept is a reasonable approximation of the grand mean
# Calculate the CoV for each posterior sample
cov_distribution <- posterior_samples %>%
  mutate(grand_mean = b_Intercept + (b_Intercept + b_time) / 2,
         cov = (sigma / grand_mean) * 100)

# Calculate the 89% credible interval for CoV
cov_89_credible_interval <- quantile(cov_distribution$cov, probs = c(0.055, 0.945))

print(cov_89_credible_interval)
```
