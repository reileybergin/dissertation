{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMU Low Back Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specs:\n",
    "- IMeasureU Blue Trident Senors\n",
    "- Sampling freq for lowg = 1125hz\n",
    "- Sampling freq for highg = 1600hz\n",
    "- 5 minutes of data = 337,500 rows (1125hz) or 478,000 (1600hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom functions ---\n",
    "import functions.file_import_gui as gui\n",
    "import functions.data_prep as prep\n",
    "import functions.custom_plots as plots\n",
    "import functions.low_back_measures as back\n",
    "import functions.peak_detection as peaks\n",
    "import functions.stats as stats\n",
    "\n",
    "# For saving files\n",
    "import os\n",
    "\n",
    "# For dataframes ---\n",
    "import pandas as pd\n",
    "\n",
    "# For plotting ---\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in IMU data files ---\n",
    "\n",
    "# Subject to process\n",
    "sub_id = 'run014'\n",
    "\n",
    "# lowg ---\n",
    "# set directory\n",
    "initialdir = f\"data/five_min_runs/{sub_id}/lowg_1125hz/back\"\n",
    "# bring in csv files with data\n",
    "dfs_lowg, keys_list = gui.read_csv_files_gui(initialdir)\n",
    "\n",
    "# highg ---\n",
    "# set directory\n",
    "initialdir = f\"data/five_min_runs/{sub_id}/highg_1600hz/back\"\n",
    "# bring in csv files with data\n",
    "dfs_highg, keys_list = gui.read_csv_files_gui(initialdir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep ---\n",
    "\n",
    "# lowg ---\n",
    "# crop data for 5 mins (removes extra rows at the beginning so that there are exactly 5 mins of data)\n",
    "prep.crop_df_five_mins(dfs_lowg, sample_freq = 1125)\n",
    "# calculate and add resultant column\n",
    "prep.add_resultant_column(dfs_lowg, column_x = 'ax_m/s/s', column_y = 'ay_m/s/s', column_z = 'az_m/s/s', name_of_res_column = 'res_m/s/s')\n",
    "# convert accel columns to gs\n",
    "prep.accel_to_gs_columns(dfs_lowg)\n",
    "# shift time scale to start at 0\n",
    "prep.shift_time_s_to_zero(dfs_lowg)\n",
    "\n",
    "\n",
    "# highg ---\n",
    "# crop data for 5 mins (removes extra rows at the beginning so that there are exactly 5 mins of data)\n",
    "prep.crop_df_five_mins(dfs_highg, sample_freq = 1600)\n",
    "# calculate and add resultant column\n",
    "prep.add_resultant_column(dfs_highg, column_x = 'ax_m/s/s', column_y = 'ay_m/s/s', column_z = 'az_m/s/s', name_of_res_column = 'res_m/s/s')\n",
    "# convert accel columns to gs\n",
    "prep.accel_to_gs_columns(dfs_highg)\n",
    "# shift time scale to start at 0\n",
    "prep.shift_time_s_to_zero(dfs_highg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean-Shift Values\n",
    "\n",
    "# lowg ---\n",
    "# meanshift\n",
    "prep.calc_mean_shift(dfs_lowg, ['ax_m/s/s', 'ay_m/s/s', 'az_m/s/s', 'ax_g', 'ay_g', 'az_g'])\n",
    "# calculate resultant from meanshifted signals\n",
    "prep.add_resultant_column(\n",
    "    dfs_lowg, column_x = 'ax_m/s/s_meanshift', column_y = 'ay_m/s/s_meanshift', column_z = 'az_m/s/s_meanshift', name_of_res_column = 'res_m/s/s_meanshift'\n",
    "    )\n",
    "prep.add_resultant_column(\n",
    "    dfs_lowg, column_x = 'ax_g_meanshift', column_y = 'ay_g_meanshift', column_z = 'az_g_meanshift', name_of_res_column = 'res_g_meanshift'\n",
    "    )\n",
    "\n",
    "# highg ---\n",
    "# meanshift\n",
    "prep.calc_mean_shift(dfs_highg, ['ax_m/s/s', 'ay_m/s/s', 'az_m/s/s', 'ax_g', 'ay_g', 'az_g'])\n",
    "# calculate resultant from meanshifted signals\n",
    "prep.add_resultant_column(\n",
    "    dfs_highg, column_x = 'ax_m/s/s_meanshift', column_y = 'ay_m/s/s_meanshift', column_z = 'az_m/s/s_meanshift', name_of_res_column = 'res_m/s/s_meanshift'\n",
    "    )\n",
    "prep.add_resultant_column(\n",
    "    dfs_highg, column_x = 'ax_g_meanshift', column_y = 'ay_g_meanshift', column_z = 'az_g_meanshift', name_of_res_column = 'res_g_meanshift'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data ---\n",
    "\n",
    "cutoff_frequency = 50 #hz\n",
    "filter_order = 4 #th\n",
    "\n",
    "# lowg ---\n",
    "sampling_frequency = 1125 #hz \n",
    "# Filter raw data\n",
    "columns_to_filter = [\n",
    "    'ax_m/s/s', 'ay_m/s/s', 'az_m/s/s', 'res_m/s/s', \n",
    "    'ax_g', 'ay_g', 'az_g', 'res_g']\n",
    "prep.apply_butter_lowpass_filter_to_dfs(dfs_lowg, columns_to_filter, sampling_frequency, cutoff_frequency, filter_order)\n",
    "# Filter mean shift data\n",
    "columns_to_filter = [\n",
    "    'ax_m/s/s_meanshift', 'ay_m/s/s_meanshift', 'az_m/s/s_meanshift', 'res_m/s/s_meanshift', \n",
    "    'ax_g_meanshift', 'ay_g_meanshift', 'az_g_meanshift', 'res_g_meanshift']\n",
    "prep.apply_butter_lowpass_filter_to_dfs(dfs_lowg, columns_to_filter, sampling_frequency, cutoff_frequency, filter_order)\n",
    "\n",
    "# highg ---\n",
    "sampling_frequency = 1600 #hz \n",
    "# Filter raw data\n",
    "columns_to_filter = [\n",
    "    'ax_m/s/s', 'ay_m/s/s', 'az_m/s/s', 'res_m/s/s', \n",
    "    'ax_g', 'ay_g', 'az_g', 'res_g']\n",
    "prep.apply_butter_lowpass_filter_to_dfs(dfs_highg, columns_to_filter, sampling_frequency, cutoff_frequency, filter_order)\n",
    "# Filter mean shift data\n",
    "columns_to_filter = [\n",
    "    'ax_m/s/s_meanshift', 'ay_m/s/s_meanshift', 'az_m/s/s_meanshift', 'res_m/s/s_meanshift', \n",
    "    'ax_g_meanshift', 'ay_g_meanshift', 'az_g_meanshift', 'res_g_meanshift']\n",
    "prep.apply_butter_lowpass_filter_to_dfs(dfs_highg, columns_to_filter, sampling_frequency, cutoff_frequency, filter_order)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create plots to save in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and store plots for specified columns ---\n",
    "\n",
    "x_col = 'time_s_scaled'\n",
    "y_cols = ['ax_m/s/s_meanshift_filtered', 'ay_m/s/s_meanshift_filtered', 'az_m/s/s_meanshift_filtered', 'res_m/s/s_meanshift_filtered']\n",
    "\n",
    "# lowg ---\n",
    "line_plots_lowg = plots.create_line_plots(dfs_lowg, x_col, y_cols)\n",
    "# highg ---\n",
    "line_plots_highg = plots.create_line_plots(dfs_highg, x_col, y_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save plots to folder ---\n",
    "\n",
    "# lowg --\n",
    "# set directory\n",
    "output_dir = f\"plots/{sub_id}/back/lowg_1125hz/\"\n",
    "# check if directory exists, if not, create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "# loop through all plots and save them\n",
    "for key, fig in line_plots_lowg.items():\n",
    "    file_path = os.path.join(output_dir, f\"{key}.html\")\n",
    "    pio.write_html(fig, file_path)\n",
    "\n",
    "# highg --\n",
    "# set directory\n",
    "output_dir = f\"plots/{sub_id}/back/highg_1600hz/\"\n",
    "# check if directory exists, if not, create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "# loop through all plots and save them\n",
    "for key, fig in line_plots_highg.items():\n",
    "    file_path = os.path.join(output_dir, f\"{key}.html\")\n",
    "    pio.write_html(fig, file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Center of Mass (CoM) Measures\n",
    "- **Root Mean Squared (RMS)** - a single value for each axis of acceleration (m/s/s) VT, ML, AP, resultant (RES)\n",
    "- **RMS Ratio** - the RMS of each axis is divided by the resultant root mean squared, for example VT_RMS/RES_RMS\n",
    "- **AVG Peak Acceleration of Resultant** - finds peaks of resultant and calculates the avg of these peaks\n",
    "\n",
    "**NOTE:** \n",
    "- The RMS and RMS Ratio are *mean-shifted* and calculated from the *filtered* signal\n",
    "- AVG Peak Accel uses the *raw* signal (no mean-shift or filtering)\n",
    "\n",
    "Axis Orientation:\n",
    "- **X-axis**: represents the **medial-lateral (ML)** direction, with positive values pointing to the right and negative values pointing to the left. This corresponds to the side-to-side movement of the body.\n",
    "- **Y-axis**: aligned with the **vertical (VT)** direction, with positive values indicating a superior (upward) direction and negative values indicating an inferior (downward) direction. This corresponds to the up and down movement of the body.\n",
    "- **Z-axis**: oriented in the **anterior-posterior (AP)** direction, with positive values pointing anterior (forward) and negative values pointing posterior (backward). This represents the forward and backward movement of the body."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS Calculations ---\n",
    "\n",
    "columns_for_rms = [\n",
    "    'ax_m/s/s', 'ay_m/s/s', 'az_m/s/s', 'res_m/s/s', # raw\n",
    "    'ax_m/s/s_filtered', 'ay_m/s/s_filtered', 'az_m/s/s_filtered', 'res_m/s/s_filtered', # filtered\n",
    "    'ax_m/s/s_meanshift_filtered', 'ay_m/s/s_meanshift_filtered', 'az_m/s/s_meanshift_filtered', 'res_m/s/s_meanshift_filtered' # mean shifted & filtered\n",
    "    ]\n",
    "# Use custom function (back.apply_rms_to_dfs)\n",
    "# returns a table in long format (variables are in a column and the values are in another)\n",
    "# also adds suffix at end of each value in the column variable (need this later to know which hz)\n",
    "rms_df_lowg = back.apply_rms_to_dfs(dfs_lowg, columns_for_rms)\n",
    "rms_df_lowg['variable'] = rms_df_lowg['variable'] + '_1125hz'\n",
    "rms_df_highg = back.apply_rms_to_dfs(dfs_highg, columns_for_rms)\n",
    "rms_df_highg['variable'] = rms_df_highg['variable'] + '_1600hz'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually Inspect Distribution of Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowg ---\n",
    "\n",
    "variables = rms_df_lowg[\"variable\"].unique()  # assuming variable is categorical\n",
    "\n",
    "fig = make_subplots(rows=3, cols=4)  # create 3x4 grid of subplots\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    row = i // 4 + 1  # calculate the row of the subplot\n",
    "    col = i % 4 + 1   # calculate the column of the subplot\n",
    "    box_data = rms_df_lowg[rms_df_lowg[\"variable\"] == var][\"value\"]\n",
    "    fig.add_trace(go.Box(y=box_data, quartilemethod='exclusive', name=var), row=row, col=col)\n",
    "\n",
    "fig.update_xaxes(title='', tickangle=0, side='top')  # moves x-axis labels to the top\n",
    "fig.update_yaxes(title='')\n",
    "fig.update_layout(title_text='Low G', height=800, showlegend=False)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# highg ---\n",
    "\n",
    "variables = rms_df_highg[\"variable\"].unique()  # assuming variable is categorical\n",
    "\n",
    "fig = make_subplots(rows=3, cols=4)  # create 3x4 grid of subplots\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    row = i // 4 + 1  # calculate the row of the subplot\n",
    "    col = i % 4 + 1   # calculate the column of the subplot\n",
    "    box_data = rms_df_highg[rms_df_highg[\"variable\"] == var][\"value\"]\n",
    "    fig.add_trace(go.Box(y=box_data, quartilemethod='exclusive', name=var), row=row, col=col)\n",
    "\n",
    "fig.update_xaxes(title='', tickangle=0, side='top')  # moves x-axis labels to the top\n",
    "fig.update_yaxes(title='')\n",
    "fig.update_layout(title_text='High G', height=800, showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table to export & append rows to to my processed variables table in Excel ---\n",
    "\n",
    "# lowg ---\n",
    "df_to_export = prep.export_tbl(rms_df_lowg)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)\n",
    "\n",
    "# highg ---\n",
    "df_to_export = prep.export_tbl(rms_df_highg)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMS Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS Ratio Calculations ---\n",
    "\n",
    "# lowg ---\n",
    "\n",
    "# filtered signal ---\n",
    "# pivot the rms df so its easier to work with\n",
    "pivot_rms_df_lowg_wide = rms_df_lowg.pivot(index='key', columns='variable', values='value')\n",
    "# calculate the ratios for each column (variable)\n",
    "for axis in ['ax_m/s/s_filtered_rms_1125hz', 'ay_m/s/s_filtered_rms_1125hz', 'az_m/s/s_filtered_rms_1125hz']:\n",
    "    pivot_rms_df_lowg_wide[axis+'_ratio'] = pivot_rms_df_lowg_wide[axis] / pivot_rms_df_lowg_wide['res_m/s/s_filtered_rms_1125hz']\n",
    "# keep only the ratio columns\n",
    "pivot_rms_df_lowg_wide = pivot_rms_df_lowg_wide[[col for col in pivot_rms_df_lowg_wide.columns if 'ratio' in col]]\n",
    "# melt the df back to a long format\n",
    "rms_filtered_ratio_df_lowg = pivot_rms_df_lowg_wide.reset_index().melt(id_vars='key', var_name='variable', value_name='value')\n",
    "\n",
    "# mean shifted filtered signal ---\n",
    "# pivot the rms df so its easier to work with\n",
    "pivot_rms_df_lowg_wide = rms_df_lowg.pivot(index='key', columns='variable', values='value')\n",
    "# calculate the ratios for each column (variable)\n",
    "for axis in ['ax_m/s/s_meanshift_filtered_rms_1125hz', 'ay_m/s/s_meanshift_filtered_rms_1125hz', 'az_m/s/s_meanshift_filtered_rms_1125hz']:\n",
    "    pivot_rms_df_lowg_wide[axis+'_ratio'] = pivot_rms_df_lowg_wide[axis] / pivot_rms_df_lowg_wide['res_m/s/s_meanshift_filtered_rms_1125hz']\n",
    "# keep only the ratio columns\n",
    "pivot_rms_df_lowg_wide = pivot_rms_df_lowg_wide[[col for col in pivot_rms_df_lowg_wide.columns if 'ratio' in col]]\n",
    "# melt the df back to a long format\n",
    "rms_meanshift_filtered_ratio_df_lowg = pivot_rms_df_lowg_wide.reset_index().melt(id_vars='key', var_name='variable', value_name='value')\n",
    "\n",
    "# combine tables above\n",
    "rms_ratio_df_lowg = pd.concat([rms_filtered_ratio_df_lowg, rms_meanshift_filtered_ratio_df_lowg], axis=0, ignore_index=True)\n",
    "\n",
    "# highg ---\n",
    "\n",
    "# filtered signal ---\n",
    "# pivot the rms df so its easier to work with\n",
    "pivot_rms_df_highg_wide = rms_df_highg.pivot(index='key', columns='variable', values='value')\n",
    "# calculate the ratios for each column (variable)\n",
    "for axis in ['ax_m/s/s_filtered_rms_1600hz', 'ay_m/s/s_filtered_rms_1600hz', 'az_m/s/s_filtered_rms_1600hz']:\n",
    "    pivot_rms_df_highg_wide[axis+'_ratio'] = pivot_rms_df_highg_wide[axis] / pivot_rms_df_highg_wide['res_m/s/s_filtered_rms_1600hz']\n",
    "# keep only the ratio columns\n",
    "pivot_rms_df_highg_wide = pivot_rms_df_highg_wide[[col for col in pivot_rms_df_highg_wide.columns if 'ratio' in col]]\n",
    "# melt the df back to a long format\n",
    "rms_filtered_ratio_df_highg = pivot_rms_df_highg_wide.reset_index().melt(id_vars='key', var_name='variable', value_name='value')\n",
    "\n",
    "# mean shifted filtered signal ---\n",
    "# pivot the rms df so its easier to work with\n",
    "pivot_rms_df_highg_wide = rms_df_highg.pivot(index='key', columns='variable', values='value')\n",
    "# calculate the ratios for each column (variable)\n",
    "for axis in ['ax_m/s/s_meanshift_filtered_rms_1600hz', 'ay_m/s/s_meanshift_filtered_rms_1600hz', 'az_m/s/s_meanshift_filtered_rms_1600hz']:\n",
    "    pivot_rms_df_highg_wide[axis+'_ratio'] = pivot_rms_df_highg_wide[axis] / pivot_rms_df_highg_wide['res_m/s/s_meanshift_filtered_rms_1600hz']\n",
    "# keep only the ratio columns\n",
    "pivot_rms_df_highg_wide = pivot_rms_df_highg_wide[[col for col in pivot_rms_df_highg_wide.columns if 'ratio' in col]]\n",
    "# melt the df back to a long format\n",
    "rms_meanshift_filtered_ratio_df_highg = pivot_rms_df_highg_wide.reset_index().melt(id_vars='key', var_name='variable', value_name='value')\n",
    "\n",
    "# combine tables above\n",
    "rms_ratio_df_highg = pd.concat([rms_filtered_ratio_df_highg, rms_meanshift_filtered_ratio_df_highg], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually Inspect Distribution of Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowg ---\n",
    "\n",
    "variables = rms_ratio_df_lowg[\"variable\"].unique()  # assuming variable is categorical\n",
    "\n",
    "fig = make_subplots(rows=2, cols=3)  # create 2x3 grid of subplots\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    row = i // 3 + 1  # calculate the row of the subplot\n",
    "    col = i % 3 + 1   # calculate the column of the subplot\n",
    "    box_data = rms_ratio_df_lowg[rms_ratio_df_lowg[\"variable\"] == var][\"value\"]\n",
    "    fig.add_trace(go.Box(y=box_data, quartilemethod='exclusive', name=var), row=row, col=col)\n",
    "\n",
    "fig.update_xaxes(title='', tickangle=0, side='top')  # moves x-axis labels to the top\n",
    "fig.update_yaxes(title='')\n",
    "fig.update_layout(title_text='Low G', height=600, showlegend=False)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# highg ---\n",
    "variables = rms_ratio_df_highg[\"variable\"].unique()  # assuming variable is categorical\n",
    "\n",
    "fig = make_subplots(rows=2, cols=3)  # create 2x3 grid of subplots\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    row = i // 3 + 1  # calculate the row of the subplot\n",
    "    col = i % 3 + 1   # calculate the column of the subplot\n",
    "    box_data = rms_ratio_df_highg[rms_ratio_df_highg[\"variable\"] == var][\"value\"]\n",
    "    fig.add_trace(go.Box(y=box_data, quartilemethod='exclusive', name=var), row=row, col=col)\n",
    "\n",
    "fig.update_xaxes(title='', tickangle=0, side='top')  # moves x-axis labels to the top\n",
    "fig.update_yaxes(title='')\n",
    "fig.update_layout(title_text='High G', height=600, showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table to export & append rows to to my processed variables table in Excel ---\n",
    "\n",
    "# lowg --\n",
    "df_to_export = prep.export_tbl(rms_ratio_df_lowg)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)\n",
    "\n",
    "# highg --\n",
    "df_to_export = prep.export_tbl(rms_ratio_df_highg)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AVG Peak Acceleration of Resultant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for functions below\n",
    "# The minimum time between peaks i.e. footstrikes corresponding to 0.25 secs (left *and* right)\n",
    "lowg_time_between_peaks = 281\n",
    "highg_time_between_peaks = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1:\n",
    "\n",
    "# Find peaks with no thresholds\n",
    "# NOTE: This will be used to then determine individual thresholds for max peak height\n",
    "\n",
    "# lowg ---\n",
    "_, dfs_res_peak_values_lowg_no_threshold = peaks.calc_avg_positive_peaks(\n",
    "    dfs_lowg, columns=['res_g'], time_column='time_s_scaled', \n",
    "    min_peak_height=None, max_peak_height=None,\n",
    "    min_samples_between_peaks=lowg_time_between_peaks\n",
    ")\n",
    "\n",
    "# highg ---\n",
    "_, dfs_res_peak_values_highg_no_threshold = peaks.calc_avg_positive_peaks(\n",
    "    dfs_highg, columns=['res_g'], time_column='time_s_scaled', \n",
    "    min_peak_height=None, max_peak_height=None,\n",
    "    min_samples_between_peaks=highg_time_between_peaks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: \n",
    "\n",
    "# Determine subject's individual thresholds for max_peak_height and min_peak_height from peaks indentified w/ no threshold\n",
    "\n",
    "k = 4 #IQR \n",
    "z = 4 #SDs\n",
    "\n",
    "# lowg ---\n",
    "# summary table w/upper bound\n",
    "summary_tbl_lowg = stats.create_summary_tbl(dfs_res_peak_values_lowg_no_threshold, ['peak_values'], k=k, z=z)\n",
    "\n",
    "# highg ---\n",
    "# summary table w/upper bound\n",
    "summary_tbl_highg = stats.create_summary_tbl(dfs_res_peak_values_highg_no_threshold, ['peak_values'], k=k, z=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3:\n",
    "\n",
    "# Use individualized max and min peak height threshold as upper limit for finding peaks\n",
    "# NOTE: This uses a different peak function that takes the summary table as inputs and steps through the indiv rows of each run/sensor\n",
    "\n",
    "# lowg ---\n",
    "res_peak_accel_lowg_df, dfs_res_peak_values_lowg_threshold = peaks.calc_avg_positive_peaks_from_tbl(\n",
    "    dfs_lowg, ['res_g'], time_column='time_s_scaled',\n",
    "    summary_table=summary_tbl_lowg, id_column=\"id\", min_peak_height_column=\"lower_bound_k\", max_peak_height_column=\"upper_bound_k\",\n",
    "    min_samples_between_peaks=lowg_time_between_peaks\n",
    "    )\n",
    "\n",
    "# highg ---\n",
    "res_peak_accel_highg_df, dfs_res_peak_values_highg_threshold = peaks.calc_avg_positive_peaks_from_tbl(\n",
    "    dfs_highg, ['res_g'], time_column='time_s_scaled',\n",
    "    summary_table=summary_tbl_highg, id_column=\"id\", min_peak_height_column=\"lower_bound_k\", max_peak_height_column=\"upper_bound_k\",\n",
    "    min_samples_between_peaks=highg_time_between_peaks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually Inspect Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot keys for specific runs to visualize below ---\n",
    "\n",
    "# lowg ---\n",
    "plot_keys_lowg = [\n",
    "    f'{sub_id}_heavyd1_prs_post_01010_lowg', \n",
    "    f'{sub_id}_heavy_prs_pre_01010_lowg', \n",
    "]\n",
    "\n",
    "# highg ---\n",
    "plot_keys_highg = [\n",
    "    f'{sub_id}_heavyd1_prs_post_01010_highg', \n",
    "    f'{sub_id}_heavy_prs_pre_01010_highg', \n",
    "]\n",
    "\n",
    "# Create dictionary with dfs I want to plot\n",
    "\n",
    "# lowg ---\n",
    "dfs_lowg_plots = {key: dfs_lowg[key] for key in plot_keys_lowg if key in dfs_lowg}\n",
    "\n",
    "# highg ---\n",
    "dfs_highg_plots = {key: dfs_highg[key] for key in plot_keys_highg if key in dfs_highg}\n",
    "\n",
    "# Create and store plots for specified columns ---\n",
    "\n",
    "x_col = 'time_s_scaled'\n",
    "y_cols = ['res_g']\n",
    "\n",
    "# lowg ---\n",
    "line_plots_lowg = plots.create_line_plots(dfs_lowg_plots, x_col, y_cols)\n",
    "\n",
    "# highg ---\n",
    "line_plots_highg = plots.create_line_plots(dfs_highg_plots, x_col, y_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowg ---\n",
    "\n",
    "peaks_to_plot = dfs_res_peak_values_lowg_threshold\n",
    "\n",
    "for key in plot_keys_lowg:\n",
    "    fig = line_plots_lowg.get(key)\n",
    "    if fig is not None:\n",
    "        # If 'peaks_2' trace already exists, remove it before adding new one\n",
    "        fig.data = [trace for trace in fig.data if trace.name != 'peaks']\n",
    "\n",
    "        # Check if key exists in peaks_to_plot\n",
    "        if key in peaks_to_plot:\n",
    "            # Get corresponding DataFrame\n",
    "            df_lowg_peaks = peaks_to_plot[key]\n",
    "            # Add points to figure\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df_lowg_peaks['time_s_scaled'], \n",
    "                y=df_lowg_peaks['peak_values'], \n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color='black',  # for example, choose a color that stands out\n",
    "                ),\n",
    "                name='peaks'  # you can name the trace to be referenced in legend\n",
    "            ))\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(f\"No plot found with key {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highg ---\n",
    "\n",
    "peaks_to_plot = dfs_res_peak_values_highg_threshold\n",
    "\n",
    "for key in plot_keys_highg:\n",
    "    fig = line_plots_highg.get(key)\n",
    "    if fig is not None:\n",
    "        # If 'peaks_2' trace already exists, remove it before adding new one\n",
    "        fig.data = [trace for trace in fig.data if trace.name != 'peaks']\n",
    "\n",
    "        # Check if key exists in peaks_to_plot\n",
    "        if key in peaks_to_plot:\n",
    "            # Get corresponding DataFrame\n",
    "            df_highg_peaks = peaks_to_plot[key]\n",
    "            # Add points to figure\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df_highg_peaks['time_s_scaled'], \n",
    "                y=df_highg_peaks['peak_values'], \n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color='black',  # for example, choose a color that stands out\n",
    "                ),\n",
    "                name='peaks'  # you can name the trace to be referenced in legend\n",
    "            ))\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(f\"No plot found with key {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Variables to Excel Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add suffixes to variables\n",
    "\n",
    "# lowg ---\n",
    "res_peak_accel_lowg_df['variable'] = res_peak_accel_lowg_df['variable'] + '_back_1125hz'\n",
    "# highg ---\n",
    "res_peak_accel_highg_df['variable'] = res_peak_accel_highg_df['variable'] + '_back_1600hz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table to export & append rows to to my processed variables table in Excel ---\n",
    "\n",
    "# lowg --\n",
    "df_to_export = prep.export_tbl(res_peak_accel_lowg_df)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)\n",
    "\n",
    "# highg --\n",
    "df_to_export = prep.export_tbl(res_peak_accel_highg_df)\n",
    "file_path = \"data/processed_variables/imu_training_load_variables.xlsx\"\n",
    "sheet_name = \"variables\"\n",
    "prep.append_df_to_excel(df_to_export, file_path, sheet_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imu_training_load_study-UQqD6JkG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
