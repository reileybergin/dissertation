---
title: "Ch.3"
subtitle: "IMU Validity & Reliability"
date: "`r format(Sys.time(), '%d %B, %Y')`" # today's date
author: "Reiley Bergin"
title-block-banner: "#1B365D"
execute:
  warning: false
  message: false
format: 
  html:
    embed-resources: true
    smooth-scroll: true
    toc: true
    toc-depth: 3
    echo: false
    code-tools: true
    link-external-newwindow: true
editor: visual
---

## Reliability

```{r}
#| label: Packages & Themes

library(tidyverse)
library(readr)
library(brms)
library(psych)
library(lme4)
library(easystats)
library(reactable)
library(openxlsx)
library(tidyr)
library(modelr)
library(ggdist)
library(tidybayes)
library(cowplot)
library(ggrepel)
library(RColorBrewer)
library(posterior)
library(distributional)

theme_set(theme_tidybayes() + panel_border())
```

```{r}
#| label: Data Prep

reliability_data <- read_csv("data/ch.3_reliability_data.csv") %>%
  
  # time pt one = 0 & time pt two = 1
  mutate(
    time_pt = case_when(
      str_detect(run_type, "time01") ~ 0,
      str_detect(run_type, "time02") ~ 1, TRUE ~ NA_real_ )) %>%
  
  mutate(
    run_type = str_extract(run_type, "[^_]+$"),  
    variable = str_replace(variable, "_500hz$", "")) %>%
  
  
  # Convert RMS valus to gs - divide value by 9.8 if variable starts with "accel"
  mutate(
    value = if_else(str_detect(variable, "^accel"), value / 9.8, value)
  ) %>%
  
  # lb = low back, rt = right tibia, lt = left tibia, 
  mutate(
    variable = str_replace_all(variable, c(
      "accel_x \\(m/s2\\)_meanshift_filtered_rms" = "lb_accel_x_rms_g",
      "accel_y \\(m/s2\\)_meanshift_filtered_rms" = "lb_accel_y_rms_g",
      "accel_z \\(m/s2\\)_meanshift_filtered_rms" = "lb_accel_z_rms_g", 
      "res_g_avg_peak_lt" = "lt_res_pk_accel_g",
      "res_g_avg_peak_rt" = "rt_res_pk_accel_g",
      "res_g_avg_peak_back" = "lb_res_pk_accel_g"
    ))
  )

# Calculate the average for the left and right tibia 
rt_lt_avg <- reliability_data %>%
  filter(variable %in% c("lt_res_pk_accel_g", "rt_res_pk_accel_g")) %>%
  group_by(sub_id, run_type, time_pt) %>%
  # NOTE: if a subject has a value for only one side (either lt or rt), 
  # and the other side is missing (NA), 
  # then the mean() function with na.rm = TRUE will calculate the mean using only the available value
  summarize(rt_lt_avg_res_pk_accel_g = mean(value, na.rm = TRUE), .groups = 'drop') %>%
  
  # format the table to match the orginal
  mutate(
    variable = "rt_lt_avg_res_pk_accel_g", 
    value = rt_lt_avg_res_pk_accel_g) %>%
  select(sub_id, run_type, variable, value, time_pt)

# Append the new rows to the original dataset
reliability_data <- bind_rows(reliability_data, rt_lt_avg) %>%
  arrange(sub_id, run_type, variable)

# List of Vaiables
vars_reliability <- reliability_data %>%
  distinct(variable) %>%
  arrange(variable) %>%
  mutate(location = case_when(
    str_detect(variable, '^(rt_|lt_|avg_)') ~ 'Tibia',
    str_detect(variable, '^lb_') ~ 'Low Back',
    TRUE ~ NA_character_)) %>%
   select(location, variable) %>%
   arrange(desc(location))
```

```{r}
#| label: Functions

# Returned filtered table in wide format

create_dfs_dict <- function(data) {
  
  # Get unique combinations of variable and run_type
  combinations <- unique(data[c("variable", "run_type")])
  
  # Initialize a list to store filtered data frames
  filtered_data_list <- list()
  
  # Loop over each combination
  for (i in 1:nrow(combinations)) {
    variable_filter <- combinations$variable[i]
    run_type_filter <- combinations$run_type[i]
    
    # Filter the data
    filtered_data <- data %>%
      filter(variable == variable_filter, run_type == run_type_filter) %>%
      select(sub_id, time_pt, value)
    
    # Store the filtered data in the list with a unique key
    key <- paste(variable_filter, run_type_filter, sep = "_")
    filtered_data_list[[key]] <- filtered_data
  }
  
  return(filtered_data_list)
}

# Calculate the range for a given normal distribution ----

calculate_range <- function(mean, std_dev) {
  # Z-scores for the lower and upper bounds (1% and 99%)
  z_score_lower <- qnorm(0.01)
  z_score_upper <- qnorm(0.99)
  
  # Calculate the range of values within the 98% interval
  interval_lower <- mean + z_score_lower * std_dev
  interval_upper <- mean + z_score_upper * std_dev
  
  return(c(interval_lower, interval_upper))
}

# Mean and SD -----

calculate_summary_stats <- function(data) {
  
  # Step 1: Calculate mean, standard deviation, and count for each combination of variable, run_type, and time_pt
  summary_stats <- data %>%
    group_by(variable, run_type, time_pt) %>%
    summarise(mean_value = mean(value, na.rm = TRUE),
              sd_value = sd(value, na.rm = TRUE),
              n = n()) %>%  # Adding count of observations
    ungroup()

  # Step 2: Pivot the table 
  final_table <- summary_stats %>%
    pivot_wider(names_from = time_pt, 
                values_from = c(mean_value, sd_value, n)) %>%
    unnest(cols = c(mean_value_0, mean_value_1,
                    sd_value_0, sd_value_1,
                    n_0, n_1)) %>%
    rename(mean_time01 = mean_value_0, 
           mean_time02 = mean_value_1,
           sd_time01 = sd_value_0, 
           sd_time02 = sd_value_1,
           n_time01 = n_0, 
           n_time02 = n_1) %>%
    select(variable, run_type, n_time01, mean_time01, sd_time01, n_time02, mean_time02, sd_time02) %>%
    mutate_if(is.numeric, round, digits = 2)

  return(final_table)
}

# Bayesian Model -----

fit_bayes_model <- function(data, variable_filter, run_type_filter, intercept_prior, b_prior) {
  
  # Filter the data based on the specified variable and run_type
  filtered_data <- data %>%
    filter(variable == variable_filter, run_type == run_type_filter)
  
  # Extract mean and sd from the provided priors
  intercept_mean <- intercept_prior[1]
  intercept_sd <- intercept_prior[2]
  b_mean <- b_prior[1]
  b_sd <- b_prior[2]
  
  # Define stanvars for intercept and b priors
  stanvars <- stanvar(intercept_mean, name='intercept_mean') +
              stanvar(intercept_sd, name='intercept_sd') +
              stanvar(b_mean, name='b_mean') +
              stanvar(b_sd, name='b_sd')
  
  # Set up priors using the custom parameters
  prs <- c(prior(normal(intercept_mean, intercept_sd), class = "Intercept"),
           prior(normal(b_mean, b_sd), class = "b"),
           prior(cauchy(0, 10), class = "sd"),
           prior(cauchy(0, 10), class = "sigma"))
  
  # Fit the Bayesian model using the filtered data
  bayes_model <- brm(
    formula = value ~ 1 + time_pt + (1 | sub_id),
    data = filtered_data,
    family = gaussian(),
    prior = prs,
    stanvars = stanvars,
    warmup = 2000, iter = 10000,
    control = list(adapt_delta = 0.98), 
    refresh = 0  # Prevent printing of progress
  )
  
  # Priors Only Model 
  #bayes_priors_only_model <- brm(
    #formula = value ~ 1 + time_pt + (1 | sub_id),
    #data = filtered_data,
    #family = gaussian(),
    #prior = prs,
    #sample_prior = "only", # priors only
    #stanvars = stanvars,
    #warmup = 2000, iter = 10000,
    #control = list(adapt_delta = 0.98), 
    #refresh = 0  # Prevent printing of progress
  #)
  
  # Create a list to return the model along with inputs for variable nd run_type
  return(list(model = bayes_model, 
              variable = variable_filter, run_type = run_type_filter))
}

# Non-Bayes MLM ---

fit_lmm_model <- function(data, variable_filter, run_type_filter) {
  
  # Filter the data based on the specified variable and run_type
  filtered_data <- data %>%
    filter(variable == variable_filter, run_type == run_type_filter)

  # Fit the linear mixed model using the filtered data
  lmm_model <- lmer(
    formula = value ~ 1 + time_pt + (1 | sub_id),
    data = filtered_data,
  )
  
  # Create a list to return the model along with inputs for variable and run_type
  return(list(model = lmm_model, variable = variable_filter, run_type = run_type_filter))
}

# ICC2,1 from psych package ---

calculate_freq_icc_psych <- function(data) {
  
  # Get unique combinations of variable and run_type
  combinations <- unique(data[c("variable", "run_type")])
  
  # Initialize an empty data frame to store results
  results <- data.frame(variable=character(), run_type=character(), icc=numeric(),
                        lower_bound=numeric(), upper_bound=numeric(), stringsAsFactors=FALSE)
  
  # Loop over each combination
  for (i in 1:nrow(combinations)) {
    variable_filter <- combinations$variable[i]
    run_type_filter <- combinations$run_type[i]
    
    # Filter the data
    filtered_data <- data %>%
      filter(variable == variable_filter, run_type == run_type_filter) %>%
      pivot_wider(names_from = time_pt, values_from = value, names_prefix = "time_pt_") %>%
      select(time_pt_0, time_pt_1) %>%
      # Remove rows where either time point is missing
      filter(!is.na(time_pt_0) & !is.na(time_pt_1))
    
    # Calculate ICC
    icc_results <- ICC(filtered_data)
    
    # Extract specific ICC values
    icc_values <- icc_results[["results"]][["ICC"]]
    
    # Extract the second ICC value (ICC,2)
    second_icc_value <- icc_values[2]
    
    # Extract lower and upper bounds
    lower_bound <- icc_results[["results"]][["lower bound"]][3]
    upper_bound <- icc_results[["results"]][["upper bound"]][3]
    
    # Create a result row
    result_row <- data.frame(variable = variable_filter,
                             run_type = run_type_filter,
                             icc2 = second_icc_value,
                             icc2_ci95_lower = lower_bound,
                             icc2_ci95_upper = upper_bound)
    
    # Bind this row to the results data frame
    results <- rbind(results, result_row)
  }
  
  return(results)
}

# Calculate ICC estimates using bayesian posterior ---

calculate_bayes_icc <- function(models_and_metadata_list) {
  
  # Function to process a single model_and_metadata
  process_single_model <- function(model_and_metadata) {
    
    # Get model
    bmod <- model_and_metadata$model

    # Extracting posterior samples
    posterior_samples <- as_draws_df(bmod, variable = c("sd_sub_id__Intercept", "sigma"))

    # Calculate ICC for each posterior sample
    icc_distribution <- posterior_samples %>%
      mutate(
        sd_intercepts = sd_sub_id__Intercept^2,
        sd_residual = sigma^2 
      ) %>%
      mutate(icc = sd_intercepts / (sd_intercepts + sd_residual))
    
    # Calculate summary statistics
    mean_icc <- mean(icc_distribution$icc)

    # Calculate 95% Credible Intervals
    ci_95_icc <- quantile(icc_distribution$icc, probs = c(0.025, 0.975))

    # Calculate 89% Credible Intervals
    ci_89_icc <- quantile(icc_distribution$icc, probs = c(0.055, 0.945))
      
    # Create a single result row
    result_row <- data.frame(variable = model_and_metadata$variable,
                             run_type = model_and_metadata$run_type,
                             icc_bayes_mean = mean_icc,
                             icc_bayes_ci95_lower = ci_95_icc[1],
                             icc_bayes_ci95_upper = ci_95_icc[2],
                             icc_bayes_ci89_lower = ci_89_icc[1],
                             icc_bayes_ci89_upper = ci_89_icc[2])

    return(result_row)
  }

  # Apply the function to each element in the list and combine results
  results <- do.call(rbind, lapply(models_and_metadata_list, process_single_model))

  return(results)
}

# Calculate ICC estimates using lmm ---

calculate_freq_icc_lmm <- function(models_and_metadata_list) {
  
  # Function to process a single model_and_metadata
  process_single_model <- function(model_and_metadata) {
    
    # Get model
    lmm <- model_and_metadata$model
    
    # Calculate summary statistics
    easystats_icc <- icc(lmm)

      
    # Create a single result row
    result_row <- data.frame(variable = model_and_metadata$variable,
                             run_type = model_and_metadata$run_type,
                             icc_lmm = easystats_icc$ICC_adjusted)

    return(result_row)
  }

  # Apply the function to each element in the list and combine results
  results <- do.call(rbind, lapply(models_and_metadata_list, process_single_model))

  return(results)
}

calculate_sem_anova <- function(data) {
  
  # Critical threshold
  Zcrit = 1.96
  
  # Get unique combinations of variable and run_type
  combinations <- unique(data[c("variable", "run_type")])
  
  # Initialize an empty data frame to store results
  results <- data.frame(variable=character(), run_type=character(), sem=numeric(),
                        md=numeric(), cov=numeric(), stringsAsFactors=FALSE)
  
  for (i in 1:nrow(combinations)) {
    variable_filter <- combinations$variable[i]
    run_type_filter <- combinations$run_type[i]
    
    # Filter the data for the specific variable and run type
    filtered_data <- data %>%
      filter(variable == variable_filter, run_type == run_type_filter)

    # Ensure each subject has entries for both time points 0 and 1
    complete_subjects <- filtered_data %>%
      group_by(sub_id) %>%
      filter(all(c(0, 1) %in% time_pt)) %>%
      ungroup()

    # Calculate Grand Mean (Mg)
    Mg <- mean(complete_subjects$value, na.rm = TRUE)

    # Conduct ANOVA
    anova_results <- aov(value ~ time_pt + Error(sub_id/time_pt), data = complete_subjects)
    mse_sub_id_time_pt <- summary(anova_results)$`Error: sub_id:time_pt`[[1]]["Residuals", "Mean Sq"]
    
    # Calculate SEM
    sem_anova <- sqrt(mse_sub_id_time_pt)

    # Calculate MD
    md_anova <- sem_anova * Zcrit * sqrt(2)

    # Calculate CoV
    cov_anova <- (sem_anova / Mg) * 100
    
    # Create a result row
    result_row <- data.frame(variable = variable_filter,
                             run_type = run_type_filter,
                             sem = sem_anova,
                             md_95 = md_anova,
                             cov = cov_anova)
    
    # Append the result row to the results data frame
    results <- rbind(results, result_row)
  }
  
  return(results)
}

# Bayes SEM and CoV ---

calculate_bayes_sem <- function(models_and_metadata_list) {
  
  # Function to process a single model_and_metadata
  process_single_model <- function(model_and_metadata) {
    
    # Get model
    bmod <- model_and_metadata$model

    # Extracting posterior samples of sigma, b_Intercept, and b_time_pt
    posterior_samples <- as_draws_df(bmod, variable = c("sigma", "b_Intercept", "b_time_pt"))

    # Estimate the average mean across time points and calculate SEM and CoV
    stats_distribution <- posterior_samples %>%
      mutate(
        estimated_mean_time_1 = b_Intercept,
        estimated_mean_time_2 = b_Intercept + b_time_pt,
        estimated_grand_mean = (estimated_mean_time_1 + estimated_mean_time_2) / 2,
        sem = sigma,
        cov = (sem / estimated_grand_mean) * 100, 
        sem_sqrt2 = sem * sqrt(2) # minimal difference formula
      )
    
    # Calculate summary statistics for SEM and CoV
    mean_sem <- mean(stats_distribution$sem)
    mean_cov <- mean(stats_distribution$cov)
    median_sem <- median(stats_distribution$sem)
    median_cov <- median(stats_distribution$cov)

    # Calculate 95% Credible Intervals
    ci_95_sem <- quantile(stats_distribution$sem, probs = c(0.025, 0.975))
    ci_95_cov <- quantile(stats_distribution$cov, probs = c(0.025, 0.975))

    # Calculate 89% Credible Intervals
    ci_89_sem <- quantile(stats_distribution$sem, probs = c(0.055, 0.945))
    ci_89_cov <- quantile(stats_distribution$cov, probs = c(0.055, 0.945))
    
    # Calculate probabilities for CoV categories
    prob_less_or_equal_10_cov <- (mean(stats_distribution$cov <= 10)) * 100
    prob_greater_10_less_or_equal_15_cov <- mean((stats_distribution$cov > 10 & stats_distribution$cov <= 15)) *100
    prob_greater_15_cov <- (mean(stats_distribution$cov > 15)) * 100
    
    # Calculate MD (minimimal difference) above specified threshold
    quantile_66_sem_sqrt2 <- quantile(stats_distribution$sem_sqrt2, probs = 0.66)
    quantile_90_sem_sqrt2 <- quantile(stats_distribution$sem_sqrt2, probs = 0.90)
    quantile_95_sem_sqrt2 <- quantile(stats_distribution$sem_sqrt2, probs = 0.95)
    quantile_99_sem_sqrt2 <- quantile(stats_distribution$sem_sqrt2, probs = 0.99)

    # Create a single result row
    result_row <- data.frame(variable = model_and_metadata$variable,
                             run_type = model_and_metadata$run_type,
                             sem_bayes_median = median_sem, 
                             sem_bayes_mean = mean_sem,
                             sem_bayes_ci95_lower = ci_95_sem[1],
                             sem_bayes_ci95_upper = ci_95_sem[2],
                             sem_bayes_ci89_lower = ci_89_sem[1],
                             sem_bayes_ci89_upper = ci_89_sem[2],
                             md_bayes_66 = quantile_66_sem_sqrt2,
                             md_bayes_90 = quantile_90_sem_sqrt2,
                             md_bayes_95 = quantile_95_sem_sqrt2,
                             md_bayes_99 = quantile_99_sem_sqrt2,
                             cov_bayes_median = median_cov,
                             cov_bayes_mean = mean_cov,
                             cov_bayes_ci95_lower = ci_95_cov[1],
                             cov_bayes_ci95_upper = ci_95_cov[2],
                             cov_bayes_ci89_lower = ci_89_cov[1],
                             cov_bayes_ci89_upper = ci_89_cov[2],
                             cov_bayes_pr_ls_or_eq_10 = prob_less_or_equal_10_cov,
                             cov_bayes_pr_gr_10_ls_or_eq_15 = prob_greater_10_less_or_equal_15_cov,
                             cov_bayes_pr_gr_15 = prob_greater_15_cov)

    return(result_row)
  }
  
  # Process each model_and_metadata and combine results
  results <- lapply(models_and_metadata_list, process_single_model)
  results_df <- do.call(rbind, results)

  return(results_df)
  
}
```

### Summary Statistics

```{r}
# table with mean and sd
stats_tbl <- calculate_summary_stats(reliability_data)
```

```{r}
# dictonary for each variable/ run_type
dfs <- create_dfs_dict(reliability_data)
```

```{r}
#| label:  Plot raw data

dfs[["lb_res_pk_accel_g_run"]] %>%
  ggplot(aes(y = sub_id, x = value, color = factor(time_pt))) +
  geom_line(aes(group = sub_id)) +  
  geom_point(size = 2) +            
  theme(legend.position = "bottom") 
```

### Bayesian Model

Model (using peak tibial acceleration as an example):

$$
\begin{align*}
\text{PTA}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i & = \alpha_{\text{subject}[i]} + \beta_{} \times \text{Time2}_i \\
\alpha        & \sim \operatorname{Normal}(10, 2.0) \\
\beta         & \sim \operatorname{Normal}(0.0, 0.50) \\
\sigma        & \sim \operatorname{HalfCauchy}(10) \\
\sigma_\alpha & \sim \operatorname{HalfCauchy}(10) \\
\end{align*}
$$ {#eq-bayes-model}

#### Prior Predictive Simulations

```{r}
# 98% of values fall within
mean <- 0.000
std_dev <- 0.500
range <- calculate_range(mean, std_dev)
range
```

```{r}
#| label: Priors

# Running ---

# Peak Tibal Acceleration (PTA) of Resultant (Sheerin 2018)
run_pta_mean <- c(9.800, 3.250)
run_pta_chg <- c(0.000, 0.500) 

# Low Back Peak Acceleration (LB-PA) of Resultant (Burke 2022)
run_lb_pa_mean <- c(3.770, 0.810)
run_lb_pa_chg <- c(0.000, 0.500) 

# Low Back RMS X (ML) (Schütte 2015)
run_lb_x_rms_mean <- c(0.520, 0.150)
run_lb_x_rms_chg <- c(0.000, 0.025)

# Low Back RMS Y (VT) (Schütte 2015)
run_lb_y_rms_mean <- c(1.390, 0.220)
run_lb_y_rms_chg <- c(0.000, 0.025)

# Low Back RMS Z (AP) (Schütte 2015)
run_lb_z_rms_mean <- c(0.510, 0.180)
run_lb_z_rms_chg <- c(0.000, 0.025)

# Walking ---

# Peak Tibal Acceleration (PTA) of Resultant (Tirosh 2019)
walk_pta_mean <- c(2.010, 0.370)
walk_pta_chg <- c(0.000, 0.500)

# Low Back Peak Acceleration (LB-PA) of Resultant (Morrow 2014)
walk_lb_pa_mean <- c(1.809, 0.411)
walk_lb_pa_chg <- c(0.000, 0.500) 

# Low Back RMS X (ML) (Henriksen 2004)
walk_lb_x_rms_mean <- c(0.138, 0.025)
walk_lb_x_rms_chg <- c(0.000, 0.013)

# Low Back RMS Y (VT) (Henriksen 2004)
walk_lb_y_rms_mean <- c(0.252, 0.024)
walk_lb_y_rms_chg <- c(0.000, 0.013)

# Low Back RMS Z (AP) (Henriksen 2004)
walk_lb_z_rms_mean <- c(0.178, 0.020)
walk_lb_z_rms_chg <- c(0.000, 0.009)
```

```{r}
#| label: Prior Predective Simulations

intercept_priors <- walk_lb_z_rms_mean 
beta_priors <- walk_lb_z_rms_chg

num_subjects <- 1000

intercept_mean <- intercept_priors[1]
intercept_sd <- intercept_priors[2]
beta_mean <- beta_priors[1]
beta_sd <- beta_priors[2]

intercept_subject <- rnorm(num_subjects, intercept_mean, intercept_sd)
beta_subject <- rnorm(num_subjects, beta_mean, beta_sd)

# Create a data frame for the random intercepts and betas
sim_data <- tibble(
  subject = 1:num_subjects,
  intercept = intercept_subject,
  beta = beta_subject ) %>% 
  mutate(time2_value = intercept + beta)

# Plot Time 2 values
ggplot(sim_data, aes(x = time2_value)) +
  geom_dotplot(binwidth = 0.02, dotsize = 0.09, fill = "#1E90FF") +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_x_continuous(name = "Time 2 Mean", limits = c(0, NA)) +
  scale_color_brewer() 
```

#### Models

```{r}
#| label: Bayes Models - Running

# Tibia Peak Accel ---
bm_run_rt_pta <- fit_bayes_model(reliability_data, "rt_res_pk_accel_g", "run", run_pta_mean, run_pta_chg)
bm_run_lt_pta <- fit_bayes_model(reliability_data, "lt_res_pk_accel_g", "run", run_pta_mean, run_pta_chg)
bm_run_avg_pta <- fit_bayes_model(reliability_data, "rt_lt_avg_res_pk_accel_g", "run", run_pta_mean, run_pta_chg)

# Low Back Peak Accel ---
bm_run_lb_pa <- fit_bayes_model(reliability_data, "lb_res_pk_accel_g", "run", run_lb_pa_mean, run_lb_pa_chg)

# Low Back RMS values ---
bm_run_lb_rms_x <- fit_bayes_model(reliability_data, "lb_accel_x_rms_g", "run", run_lb_x_rms_mean, run_lb_x_rms_chg)
bm_run_lb_rms_y <- fit_bayes_model(reliability_data, "lb_accel_y_rms_g", "run", run_lb_y_rms_mean, run_lb_y_rms_chg)
bm_run_lb_rms_z <- fit_bayes_model(reliability_data, "lb_accel_z_rms_g", "run", run_lb_z_rms_mean, run_lb_z_rms_chg)
```

```{r}
#| label: Bayes Models - Walking

# Walking ---

# Tibia Peak Accel ---
bm_walk_rt_pta <- fit_bayes_model(reliability_data, "rt_res_pk_accel_g", "walk", walk_pta_mean, walk_pta_chg)
bm_walk_lt_pta <- fit_bayes_model(reliability_data, "lt_res_pk_accel_g", "walk", walk_pta_mean, walk_pta_chg)
bm_walk_avg_pta <- fit_bayes_model(reliability_data, "rt_lt_avg_res_pk_accel_g", "walk", walk_pta_mean, walk_pta_chg)

# Low Back Peak Accel ---
bm_walk_lb_pa <- fit_bayes_model(reliability_data, "lb_res_pk_accel_g", "walk", walk_lb_pa_mean, walk_lb_pa_chg)

# Low Back RMS values ---
bm_walk_lb_rms_x <- fit_bayes_model(reliability_data, "lb_accel_x_rms_g", "walk", walk_lb_x_rms_mean, walk_lb_x_rms_chg)
bm_walk_lb_rms_y <- fit_bayes_model(reliability_data, "lb_accel_y_rms_g", "walk", walk_lb_y_rms_mean, walk_lb_y_rms_chg)
bm_walk_lb_rms_z <- fit_bayes_model(reliability_data, "lb_accel_z_rms_g", "walk", walk_lb_z_rms_mean, walk_lb_z_rms_chg)
```

#### Model Checking

1.  **R-hat ≈ 1.0**: Indicates that the chains have converged to a common distribution. The closer R-hat is to 1.0, the better.

2.  **R-hat ≤ 1.01**: Usually considered acceptable. Values in this range suggest that convergence is likely adequate, and the MCMC samples can be trusted for inference.

3.  **R-hat \> 1.01**: Indicates potential issues with convergence. If R-hat is significantly greater than 1.01, it suggests that the chains have not mixed well, and the MCMC simulation may not have adequately explored the posterior distribution. In such cases, it's advisable to investigate further, which might involve running more iterations, adjusting the model, or improving the tuning of the MCMC algorithm

```{r}
model<-bm_walk_lb_rms_z$model
table <- dfs[["lb_accel_z_rms_g_walk"]]
```

```{r}
# Shiny app from rstan for all the things
launch_shinystan(model)
```

```{r}
# Model summary
summary(model)
```

```{r}
# View intercepts for each subject
coef(model)
```

```{r}
# Prior Predictive Check 
# NOTE: This doesn't make sense because my priors for sigmas are very uniformative. I will come back to this later

#table %>%
  #add_predicted_draws(priors_model)

```

```{r}
# Intercepts of each subject

low <- 6
high <- 12

model %>%
  spread_draws(b_Intercept, r_sub_id[sub_id,]) %>%
  mutate(sub_id_mean = b_Intercept + r_sub_id) %>%
  ggplot(aes(y = sub_id, x = sub_id_mean, fill = after_stat(abs(x) > high | abs(x) < low))) +
  stat_halfeye() +
  geom_vline(xintercept = c(low, high), linetype = "dashed") +
  scale_fill_manual(values = c("gray80", "skyblue"))
```

```{r}

# Posterior Predictions for each subject
table %>%
  data_grid(sub_id, time_pt) %>%
  add_predicted_draws(priors_model) %>%
  ggplot(aes(x = .prediction, y = sub_id)) +
  stat_slab()
```

```{r}

# Posterior Predictions, Quantile dotplots
table%>%
  data_grid(sub_id, time_pt) %>%
  add_epred_draws(model) %>%
  ggplot(aes(x = .epred, y = sub_id, fill = after_stat(x < 0))) +
  stat_dotsinterval(quantiles = 100) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  scale_fill_manual(values = c("gray80", "skyblue"))
```

```{r}

# Posterior predictions, Kruschke-style
table%>%
  data_grid(sub_id, time_pt) %>%
  add_epred_draws(model, dpar = c("mu", "sigma")) %>%
  sample_draws(30) %>%
  ggplot(aes(y = sub_id)) +
  stat_slab(aes(xdist = dist_normal(mu, sigma)), 
    slab_color = "gray65", alpha = 1/10, fill = NA
  ) +
  geom_point(aes(x = value), data = table, shape = 21, fill = "#9ECAE1", size = 2)
```

### Regular Linear Multilevel Model

```{r}
#| label: LMM Models - Running

# Running ---

lmm_run_rt_pta <- fit_lmm_model(reliability_data, "rt_res_pk_accel_g", "run")
lmm_run_lt_pta <- fit_lmm_model(reliability_data, "lt_res_pk_accel_g", "run")
lmm_run_avg_pta <- fit_lmm_model(reliability_data, "rt_lt_avg_res_pk_accel_g", "run")
lmm_run_lb_pa <- fit_lmm_model(reliability_data, "lb_res_pk_accel_g", "run")
lmm_run_lb_rms_x <- fit_lmm_model(reliability_data, "lb_accel_x_rms_g", "run")
lmm_run_lb_rms_y <- fit_lmm_model(reliability_data, "lb_accel_y_rms_g", "run")
lmm_run_lb_rms_z <- fit_lmm_model(reliability_data, "lb_accel_z_rms_g", "run")
```

```{r}
#| label: LMM Models - Walking

# Walking ---

lmm_walk_rt_pta <- fit_lmm_model(reliability_data, "rt_res_pk_accel_g", "walk")
lmm_walk_lt_pta <- fit_lmm_model(reliability_data, "lt_res_pk_accel_g", "walk")
lmm_walk_avg_pta <- fit_lmm_model(reliability_data, "rt_lt_avg_res_pk_accel_g", "walk")
lmm_walk_lb_pa <- fit_lmm_model(reliability_data, "lb_res_pk_accel_g", "walk")
lmm_walk_lb_rms_x <- fit_lmm_model(reliability_data, "lb_accel_x_rms_g", "walk")
lmm_walk_lb_rms_y <- fit_lmm_model(reliability_data, "lb_accel_y_rms_g", "walk")
lmm_walk_lb_rms_z <- fit_lmm_model(reliability_data, "lb_accel_z_rms_g", "walk")
```

### Intraclass Correlation Coefficient (ICC)

Using `ICC()` from [psych](https://cran.r-project.org/web/packages/psych/psych.pdf) package and `icc()`from the [easystats](https://github.com/easystats/easystats) package.

#### Frequentist Approach

```{r}
# Looking at ICC 2,1 from psych package
freq_icc_pysch_tbl <- calculate_freq_icc_psych(reliability_data)
```

```{r}
# Using LMM and easystats icc 
freq_icc_lmm_tbl <- calculate_freq_icc_lmm(list(
  lmm_run_avg_pta, lmm_run_rt_pta, lmm_run_lt_pta, 
  lmm_run_lb_pa, lmm_run_lb_rms_x, lmm_run_lb_rms_y, lmm_run_lb_rms_z, 
  lmm_walk_avg_pta, lmm_walk_rt_pta, lmm_walk_lt_pta, 
  lmm_walk_lb_pa, lmm_walk_lb_rms_x, lmm_walk_lb_rms_y, lmm_walk_lb_rms_z))
```

#### Bayesian Approach

```{r}
# Calculates posterior distribution and samples ICC values
bayes_icc_tbl <- calculate_bayes_icc(list(
  bm_run_avg_pta, bm_run_rt_pta, bm_run_lt_pta, 
  bm_run_lb_pa, bm_run_lb_rms_x, bm_run_lb_rms_y, bm_run_lb_rms_z,
  bm_walk_avg_pta, bm_walk_rt_pta, bm_walk_lt_pta, 
  bm_walk_lb_pa, bm_walk_lb_rms_x, bm_walk_lb_rms_y, bm_walk_lb_rms_z))
```

### Standard Error of Measurement (SEM), MD (Minimal Difference), & Coefficient of Variation (CoV)

#### Frequentist Approach

The SEM was calculated from the *mean square error term (MSE) of a repeated measures ANOVA* as described Weir & Vincent in [Statistics in Kinesiology 5th Edition.](#0)

$$
\begin{align*}
\text{SEM} & = \sqrt{\text{MSE}} \\
\text{MD}  & = \text{SEM} \times Z_{\text{crit}} \times \sqrt{2} \\
\text{CoV} & = \left( \frac{\text{SEM}}{\text{Grand Mean}} \right) \times 100 \\
\end{align*}
$$ {#eq-absolute-reliability}

```{r}
# Calculates SEM, MD, and CoV 
freq_sem_tbl <- calculate_sem_anova(reliability_data)
```

#### Bayesian Approach

**Calculating SEM:**

1.  **Extract Posterior Samples:** Obtain the posterior samples for the residual standard deviation (**`sigma`**) from the Bayesian model. **`sigma`** in a Bayesian regression model typically represents the standard deviation of residuals, analogous to the root of Mean Squared Error in frequentist models.

2.  **SEM Calculation:** Directly use the posterior samples of **`sigma`** as estimates of SEM. In this context, SEM is considered to be the standard deviation of the measurement error.

**Calculating CoV:**

1.  **Estimate Mean:** Determine an average mean value for use in the CoV calculation. In the absence of a direct grand mean (Mg) across all observations, use an estimated mean. For example, calculate the average of the model's intercept (mean at Time 1) and the sum of the intercept and beta coefficient (mean at Time 2).

2.  **Extract Posterior Samples:** Extract the posterior samples for the intercept (**`b_Intercept`**), the slope or beta coefficient for time (**`b_time_pt`**), and **`sigma`**.

3.  **Compute Average Mean and CoV:** For each set of posterior samples, calculate the estimated average mean across time points. Then, compute CoV as the ratio of SEM (posterior **`sigma`**) to the estimated average mean, multiplied by 100 to express it as a percentage.

**Credible Intervals:**

The 95% and 89% credible intervals for both the Standard Error of Measurement (SEM) and Coefficient of Variation (CoV) are calculated from the posterior distribution by drawing samples, providing a range of values within which the true SEM and CoV are likely to fall with 95% and 89% probability, respectively.

```{r}

# Calculate SEM and CoV using parameters from posterior 
bayes_sem_tbl <- calculate_bayes_sem(list(
  bm_run_avg_pta, bm_run_rt_pta, bm_run_lt_pta, 
  bm_run_lb_pa, bm_run_lb_rms_x, bm_run_lb_rms_y, bm_run_lb_rms_z, 
  bm_walk_avg_pta, bm_walk_rt_pta, bm_walk_lt_pta, 
  bm_walk_lb_pa, bm_walk_lb_rms_x, bm_walk_lb_rms_y, bm_walk_lb_rms_z))
```

```{r}
#| label: Export Reliability Results

# Full Table
results_reliability <- stats_tbl %>%
  inner_join(freq_icc_pysch_tbl, by = c("variable", "run_type")) %>%
  inner_join(freq_icc_lmm_tbl, by = c("variable", "run_type")) %>%
  inner_join(bayes_icc_tbl, by = c("variable", "run_type")) %>%
  inner_join(freq_sem_tbl, by = c("variable", "run_type")) %>%
  inner_join(bayes_sem_tbl, by = c("variable", "run_type")) %>%
  pivot_longer(
    cols = -c(variable, run_type), 
    names_to = "statistic",        
    values_to = "value") %>%
  mutate(value = round(value, 3))

# Write to an Excel file
write.xlsx(results_reliability, file = "ch.3_results/results_reliability.xlsx")
```

## Validity

```{r}

# Packages
library(tidyverse)
library(easystats)
library(readr)
library(openxlsx)
```

```{r}

# Running data ---

validity_run_data <- read_csv("data/ch.3_validity_data.csv") %>%
  filter(run_type == "run") %>%
  select(-run_type, -leg) %>%
  pivot_wider(names_from = variable, values_from = value) %>%
  # Remove '_500hz'
  rename_with(~ gsub("_500hz", "", .), .cols = everything())  


# Walking data ---

validity_walk_data <- read_csv("data/ch.3_validity_data.csv") %>%
  filter(run_type == "walk") %>%
  select(-run_type, -leg) %>%
  pivot_wider(names_from = variable, values_from = value) %>%
  # Remove '_500hz'
  rename_with(~ gsub("_500hz", "", .), .cols = everything())  

```

### Pearson Correlation

Using `correlation` from the [easystats](https://github.com/easystats/easystats) package.

#### Frequentist Approach Only

```{r}
#| label: Pearson correlation for each combination

# Running
run_correlations_tbl <- correlation(validity_run_data)

# Walking 
walk_correlations_tbl <- correlation(validity_walk_data)
```

```{r}
# Scatter Plots

# Running
plot(cor_test(validity_run_data, "max_grf_mag_z_bm", "max_knee_force_z"))

# Walking
plot(cor_test(validity_walk_data, "max_grf_mag_z_bm", "max_knee_force_z"))
```

```{r}
#| label: Export Validity Results

# Write the matrix run_correlations_tbl to a CSV file
write.csv(run_correlations_tbl, "ch.3_results/validity_run_corr_matrix.csv", row.names = TRUE)
write.csv(walk_correlations_tbl, "ch.3_results/validity_walk_corr_matrix.csv", row.names = TRUE)
```
